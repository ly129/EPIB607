# Probability

## Introduction[^1] {-}

The random variation associated with _measurement_ procedures in a scientific analysis requires a framework in which the **uncertainty** and **variability** that are inherent in the procedure can be handled. The key goal of Probability and Statistical modelling is to establish a mathematical framework within which _random_ variation (due, for example, to experimental error or natural variation) can be quantified so that
_systematic_ variation (arising due to potentially important biological differences) can be studied.  

[^1]: Reproduced with permission from http://www.math.mcgill.ca/dstephens/


Broadly, the \textit{Scientific Process} involves several different stages:  

\begin{equation*}
\begin{array}{cl}
\text{{THEORETICAL MODELLING}} & \rightarrow \text{{MATHEMATICAL/PROBABILISTIC MODELLING}} \\
\downarrow &  \\
\text{{PREDICTION}} &  \\
\downarrow &  \\
\text{{EXPERIMENTATION/OBSERVATION}} &  \\
\downarrow &  \\
\text{{VALIDATION}} &
\end{array}
\end{equation*}  


_Mathematical/Probabilistic_ _modelling_ facilitates PREDICTION; _Statistical Analysis_ provides the means of validation
of predicted behaviour.  

To explain the variation in observed data, we need to introduce the concept of a _probability distribution_. Essentially we need
to be able to model, or specify, or compute the _chance_ of observing the data that we collect or expect to collect. This will then allow us to assess how likely the data were to occur by chance alone, that is, how _surprising_ the observed data are in light of an assumed theoretical model.

For example, consider two nucleotide sequences of the same length that we wish to assess for similarity:

```{example, name="Two nucleotide sequences"}
\begin{equation*}
\begin{array}{ll}
\text{{Sequence 1}}{\qquad } & ATAGTAGATACGCACCGAGGA \\
&  \\
\text{{Sequence 2}}{\qquad } & ATCTTAGATAGGCACTGAGGA
\end{array}
\end{equation*}

How can we assess sequence similarity formally ? The number of discordant positions is 4, but how informative is that summary measure ? Perhaps we need to assess the chance, for example, that a point mutation $$ A\rightarrow C $$ occurs (as in the discordant position 3) in unit evolutionary time. Perhaps the chance of observing a sub-sequence
\begin{equation*}
ATCTTA
\end{equation*}
rather than
\begin{equation*}
ATAGTA
\end{equation*}
(in positions 1-6) is important. 

- Is the hidden (or _latent_) structure in the sequence, corresponding to whether the sequence originates from a coding region or otherwise, important ? 
- Can we even infer the hidden structure in light of the data we have observed ?  

```

These questions can only really be answered when we have an understanding of randomness and variation. The framework that we will use to pose and answer such questions formally is given to us by _probability theory_.  


### Probability: A Measure of Uncertainty[^2]  {-}

Often in life we are confronted by our own ignorance. Whether we are pondering tonight's traffic jam, tomorrow's weather, next week's stock prices, an upcoming election, or where we left our hat, often we do not know an outcome with certainty. Instead, we are forced to guess, to estimate, to hedge our bets. 

> Probability is the science of uncertainty. 

It provides precise mathematical rules for understanding and analyzing our own ignorance. It does not tell us tomorrow's weather
or next week's stock prices; rather, it gives us a **framework for working with our limited knowledge** and for **making sensible decisions based on what we do and do not know**.    

To say there is a 40% chance of rain tomorrow is not to know tomorrowâ€™s weather. Rather, it is to **know what we do not know** about tomorrow's weather.
In this course, we will develop a more precise understanding of what it means to say there is a 40% chance of rain tomorrow. We will learn how to work with ideas of randomness, probability, expected value, prediction, estimation, etc., in ways that are sensible and mathematically clear.


[^2]: http://www.utstat.toronto.edu/mikevans/jeffrosenthal/book.pdf


## Sample Spaces and Events

### Sample Spaces

```{definition, name="Sample Space"}
The sample space $\Omega$ is the set of possible outcomes of an experiment. Points $\omega$ in $\Omega$ are called sample outcomes, realizations, or elements. 
```

```{example, name="Coin tossing"}
$\Omega = \left\lbrace  H, T  \right\rbrace$
```

```{example, name="Dice"}
$\Omega = \left\lbrace  1,2,3,4,5,6  \right\rbrace$
```

```{example, name="Proportions"}
$\Omega = \left\lbrace  x : 0 \leq x \leq 1  \right\rbrace$
```

```{example, name="Time measurement"}
$\Omega = \left\lbrace  x :  x > 0  \right\rbrace = {\mathbb{R}}^{+}$
```


```{example, name="Temperature measurement"}
$\Omega = \left\{ x:a\leq x\leq b\right\} \subseteq { \mathbb{R}}$
```


```{example, name="Biological Sequence Analysis"}
The experiment may involve the observation of a nucleotide or protein sequence, so that the sample space $\Omega$ may comprise all sequences (of bases/amino acids) up to a given length, and a sample outcome would be a particular observed sequence.
```

There are two basic types of experiment:
- Counting
- Measurement  

We shall see that these two types lead to two distinct ways of specifying probability distributions.

The collection of sample outcomes is a **set** (a collection of items) written as
\begin{equation*}
s\in \Omega
\end{equation*}
if $s$ _is a member of the set_ $\Omega$.

### Events

```{definition, name="Event"}
An event $E$ is a subset of the sample space $\Omega$ ($E \subseteq \Omega$). Events are usually denoted by upper case letters near the beginning of the alphabet, like $A, B, C$. An event which consists of only one outcome is called a simple (or elementary event); otherwise it is a compound event.
```


```{block2, type='rmdnote'}
The sets $\Omega$ and $E$ can be either be written as a list of items, for example,
\begin{equation*}
E=\left\{ s_{1},s_{2},...,s_{n},...\right\}
\end{equation*}
which may a finite or infinite list, or can only be represented by a continuum of outcomes, for example
\begin{equation*}
E=\left\{ x:0.6<x\leq 2.3\right\}
\end{equation*}
```


Events are manipulated using **set theory** notation; if $A$ and $B$ are two events, $A, B \subseteq \Omega$, then

- $A \cup B$ is the set of outcomes that belong to $A$ **or** to $B$, or to both,
- $A \cap B$ is the set of outcomes that belong to both $A$ **and** to $B$.
- $A^c$ (complement of $A$) is the set of outcomes **not** in $A$
- $A \backslash B = A \cap B^c$ 


The empty event will be denoted by $\varnothing$. Two events $A$ and $B$ are mutually exclusive if $A \cap B = \varnothing$, i.e., the collection of sample outcomes have no element in common. 


![](images/venn.png)


## Axioms, Interpretations, and Properties of Probability

```{definition, name="Axioms (basic properties) of Probability"}
To ensure that the probability assignments will be consistent with our intuitive notions of probability, all assign-
ments should satisfy the following axioms (basic properties) of probability  

- **AXIOM 1:** For any event $A$, \[ P(A) \geq 0 \]
- **AXIOM 2:** \[ P(\Omega) = 1 \]
- **AXIOM 3:** If $A_1, A_2, \ldots$ is an infinite collection of disjoint events, then
\[   P\left(A_1 \cup A_2 \cup \cdots \right) = \sum_{i=1}^{\infty} P(A_i)  \]

```


```{proposition}
$P(\varnothing) = 0$ where $\varnothing$ is the null event. This in turn implies that the property contained in Axiom 3 is valid for a finite collection of events.
```



```{proposition}
For any event $A$,  
\[ P(A) = 1 - P(A^c) \]
```


```{proposition}
For any event $A$,  
\[ P(A) \leq 1 \]
```


```{proposition}
For any events $A$ and $B$,   
\[ P(A \cup B) = P(A) + P(B) - P(A \cap B) \]
```



## Counting Techniques

When the various outcomes of an experiment are equally likely (the same probability is assigned to each simple event), the task of computing probabilities reduces to counting. In particular, if $N$ is the number of outcomes in a sample space and
$N(A)$ is the number of outcomes contained in an event $A$, then  \[ P(A) = \frac{N(A)}{N} \]


```{proposition, name="Product rule for ordered pairs"}
If the first element or object of an ordered pair can be selected in $n_1$ ways, and for each of these $n_1$ ways the second element of the pair can be selected in $n_2$ ways, then the number of pairs is $n_1 \cdot n_2$.
```

### Permutations

```{definition, name="Permutation"}
Any ordered sequence of $k$ objects taken from a set of $n$ distinct objects is called a permutation of size $k$ of the objects. The number of permutations of size $k$ that can be constructed from the $n$ objects is denoted by $P_k^n$:  

\[ P_k^n = \frac{n!}{(n-k)!} \]
```


### Combinations

```{definition, name="Combination"}
Given a set of $n$ distinct objects, any unordered subset of size $k$ of the objects is called a combination. The number of combinations $n$ of size $k$ that can be formed from $n$ distinct objects will be denoted by  

\[ \binom{n}{k} = \frac{n!}{k!(n-k)!} = \frac{P_k^n}{k!} \]
```


## Conditional Probability

Conditional probability is the means by which probabilities are updated in the light of new information. We examine how the information _an event B has occurred_ affects the probability assigned to $A$. 


```{example, name=""}

```



```{definition, name="Conditional Probability"}


```


## Independence



