\documentclass{beamer}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}

\usepackage{default}
\usepackage{animate} %need the animate.sty file 
\usepackage{graphicx}
%\graphicspath{{/home/sahir/Dropbox/jobs/laval/minicours/slides/}}
\usepackage{hyperref, url}
%\usepackage[round,sort]{natbib}   % bibliography omit 'round' option if you prefer square brackets
%\bibliographystyle{apalike}
\usepackage{biblatex}
\bibliography{bib.bib}
% Removes icon in bibliography
\setbeamertemplate{bibliography item}[text]

\usepackage[normalem]{ulem}


\usepackage[figurename=Fig.]{caption}
\usepackage{subfig}
\usepackage{tikz, pgfplots,epsfig}
\usetikzlibrary{arrows,shapes.geometric}
\usepackage{color, colortbl,xcolor}
\definecolor{lightgray}{RGB}{200,200,200}
\definecolor{myblue}{RGB}{0,89,179}
\usepackage{comment}
\setbeamercolor{frametitle}{fg=myblue}
\setbeamercolor{section in head/foot}{bg=myblue, fg=white}
\setbeamercolor{author in head/foot}{bg=myblue}
\setbeamercolor{date in head/foot}{bg=myblue}

\usepackage{array}
\newcolumntype{L}{>{\centering\arraybackslash}m{3cm}} % used for text wrapping in ctable
\usepackage{ctable}
\usepackage[utf8]{inputenc}
\usepackage{fontenc}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\def\widebar#1{\overline{#1}}
\definecolor{whitesmoke}{rgb}{0.96, 0.96, 0.96}

\usepackage{amssymb}
\usepackage{amsmath}

\usepackage{bm}
\def\transpose{{\sf{T}}}
\def\E{{\skew0\bm{E}}}
\def\Xvec{{\skew0\bm{X}}}
\def\Xveca{{\skew0\bm{X}}_1}
\def\Xvecb{{\skew0\bm{X}}_2}

\def\Yvec{{\skew0\bm{Y}}}
\def\bmY{{\skew0\bm{Y}}}
\def\bmX{{\skew0\bm{X}}}
\def\bmy{{\skew0\bm{y}}}
\def\bmG{{\skew0\bm{G}}}
\def\bmS{{\skew0\bm{S}}}
\def\bmA{{\skew0\bm{A}}}
\def\bmB{{\skew0\bm{B}}}
\def\bmD{{\skew0\bm{D}}}
\def\bmI{{\skew0\bm{I}}}
\def\bmV{{\skew0\bm{V}}}
\def\bmU{{\skew0\bm{U}}}
\def\bv{{\skew0\bm{v}}}
\def\bw{{\skew0\bm{w}}}
\def\bmm{{\skew0\bm{m}}}
\def\bmzero{{\skew0\bm{0}}}
\def\bx{{\skew0\bm{x}}}
\def\xveca{{\skew0\bm{x}}_1}
\def\xvecb{{\skew0\bm{x}}_2}

\def\N{{\skew0\mathcal{N}}}
\def\T{{\small T}}

\def\mvec{{\skew0\bm{m}}}
\def\bmmu{{\skew0\bm{\mu}}}
\def\muvec{{\skew0\bm{\mu}}}
\def\balpha{{\skew0\bm{\alpha}}}
\def\bbeta{{\skew0\bm{\beta}}}
\def\bmtheta{{\skew0\bm{\theta}}}
\def\btheta{{\skew0\bm{\theta}}}

\def\cvec{{\skew0\mathbf{c}}}

\def\Xbar{\overline{X}}

\definecolor{lightgray}{rgb}{0.91,0.91,0.91}
\definecolor{purpleblue}{rgb}{0.50,0.50,1.00}



\usepackage{fontspec}
%\setsansfont{Fira Sans}
%\setmonofont{Fira Mono}
\setsansfont[ItalicFont={Fira Sans Light Italic},BoldFont={Fira Sans},BoldItalicFont={Fira Sans Italic}]{Fira Sans Light}
\setmonofont[BoldFont={Fira Mono Medium}]{Fira Mono}


\setbeamercolor{itemize item}{fg=myblue}
\setbeamertemplate{itemize item}[square]

\setbeamertemplate{navigation symbols}{\usebeamercolor[fg]{title in head/foot}\usebeamerfont{title in head/foot}\insertframenumber}
\setbeamertemplate{footline}{}

\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{exercise}[theorem]{Exercise}

\titlegraphic{\hfill\includegraphics[height=1cm]{mcgill_logo.png}}


%% You also use hyperref, and pick colors 
\hypersetup{colorlinks,citecolor=orange,filecolor=red,linkcolor=brown,urlcolor=blue}

\newcommand {\framedgraphiccaption}[2] {
	\begin{figure}
		\centering
		\includegraphics[width=\textwidth,height=0.8\textheight,keepaspectratio]{#1}
		\caption{#2}
	\end{figure}
}

\newcommand {\framedgraphic}[1] {
	\begin{figure}
		\centering
		\includegraphics[width=\textwidth,height=0.9\textheight,keepaspectratio]{#1}
	\end{figure}
}


\AtBeginSection[]{
	\begin{frame}
		\vfill
		\centering
		\begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
			\usebeamerfont{title}\insertsectionhead\par%
		\end{beamercolorbox}
		\vfill
	\end{frame}
}

\newcommand\Wider[2][3em]{%
	\makebox[\linewidth][c]{%
		\begin{minipage}{\dimexpr\textwidth+#1\relax}
			\raggedright#2
		\end{minipage}%
	}%
}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}
%\sffamily



%\title{Introduction to Regression Trees}
%\author{Sahir Bhatnagar \inst{1}}
%\author[shortname]{Sahir Rai Bhatnagar, PhD Candidate (Biostatistics) }
%\institute[shortinst]{Department of Epidemiology, Biostatistics and Occupational Health}

\title{Sampling Distributions and the Central Limit Theorem (CLT)}
\author{Sahir Bhatnagar and James Hanley}
\institute{
	EPIB 607\\
	Department of Epidemiology, Biostatistics, and Occupational Health\\
	McGill University\\
	
	\vspace{0.1 in}
	
	\texttt{sahir.bhatnagar@mcgill.ca}\\
	\texttt{\url{https://sahirbhatnagar.com/EPIB607/}}}

%\date

\maketitle

\section{Parameters,  Samples,  and  Statistics}

\begin{frame}{Parameters,  Samples,  and  Statistics}
\begin{itemize}
		\item \textbf{Paramter}: An  unknown  numerical  constant  pertaining  to  a  population/universe,  or  in  a  statistical  model. 
		\begin{itemize}
			\item $\mu$: population mean $\qquad\qquad$ $\pi$: population proportion
		\end{itemize}
	\pause
		\item \textbf{Statistic}: A  numerical  quantity  calculated  from  a  sample. The  empirical counterpart of the parameter,  used  to  \textit{estimate}  it.
		\pause
			\begin{itemize}
			\item $\bar{y}$: sample mean $\qquad\qquad$ $p$: sample proportion
		\end{itemize}
\end{itemize}

\pause
\Wider[4em]{
	\centering
	\includegraphics[scale=0.35]{MeansFig1.png}
}




\end{frame}


\begin{frame}{Examples}
\textbf{Proportions}:
	\begin{itemize}
	  \setlength\itemsep{1.2em}
	  \item Proportion of  Earth's  surface  covered  by  water
	  \item Proportion  who saw a medical doctor last year
	  \item Proportion of Québécois who don't have a family doctor
		\end{itemize}  
\pause 
	\vspace{0.1 in}
	
\textbf{Means}:
\begin{itemize}	 
	  \setlength\itemsep{1.2em} 
		\item Mean  depth  in $n$ randomly  selected  ocean  locations
		\item Mean  household  size  in $n$ randomly  selected  households.  
		\item Median  number  of  persons  under-5  in  a  sample  of $n$ households
	\end{itemize}  

\end{frame}

\frame{\frametitle{Samples must be random} 
	
\begin{itemize}
	\item 	The validity of inference will depend on the
	way that the sample was collected. If a sample was collected badly, no amount of
	statistical sophistication can rescue the study. \\ \ \\ \pause
	
	\item Samples should be \textbf{random}. That is, there should be no systematic set of
	characteristics that is related to the scientific question of interest that causes some
	people to be more likely to be sampled than others. The simplest type of randomization
	selects members from the population with equal probability (a uniform distribution). \\ \
	\\ \pause
	
	\item When conducting a study, it is always better to seek statistical
	advice sooner rather than later. Get a statistician involved at the
	\textit{planning} stage of the study... by the analysis stage, it
	may be too late!
\end{itemize}
}


\frame{\frametitle{Samples must be random - No cheating!} 
	
	\textbf{Do not cheat by} \pause 
	\begin{itemize}
		\item 	Taking 5 people from the \emph{same} household to estimate
		
		 \begin{itemize}
		 	\item proportion of Québécois who don't have a family doctor
		 	\item who saw a medical doctor last year
		 	\item average rent
		 \end{itemize}  
	 
	 \pause
	 	\vspace{0.2 in}
		
		\item Sampling the depth of the ocean \emph{only around Montreal} to estimate \begin{itemize}
			\item proportion of  Earth's  surface  covered  by  water
		\end{itemize} 

	\end{itemize}
}



\frame{\frametitle{Collecting data takes effort} 
	
	\textbf{In general} 
	\begin{itemize}
		\item The larger the sample $\to $ the more accurate the estimate (if sampling is done correctly) \pause
		
		
	\end{itemize}	
		
			 	\vspace{0.2 in}
			 	
			 		\textbf{CAVEAT} 
			 	\begin{itemize}
			 		\item Collecting more data takes effort and money!
			 		\item We will also soon discover the curse of the $\sqrt{n}$ 			 		
			 	\end{itemize}	

}


\frame{\frametitle{Collecting data takes effort} 
	
	\textbf{In general} 
	\begin{itemize}
		\item The larger the sample $\to $ the more accurate the estimate (if sampling is done correctly) \pause
		
		
	\end{itemize}	
	
	\vspace{0.2 in}
	
	\textbf{CAVEAT} 
	\begin{itemize}
		\item Collecting more data takes effort and money!
		\item We will also soon discover the curse of the $\sqrt{n}$ 			 		
	\end{itemize}	
	
}



\section{Sampling Distributions}


\frame{\frametitle{Sampling Distributions} 
	
	\begin{itemize}
				  \setlength\itemsep{2em} 
		\item Given a sample of $n$ observations from a population, we will be
		calculating estimates of the population mean, proportion, standard
		deviation, and various other population characteristics
		(parameters) \pause
		
	\item Prior to obtaining data, there is uncertainty as to which of all
		possible samples will occur \pause

\item  Because of this, estimates such as $\bar{y}$ (the sample mean) will vary
		from one sample to another
		
		
	\end{itemize}	

	
}


\frame{\frametitle{Sampling Distributions} 
	
	\begin{itemize}
		\setlength\itemsep{2em} 
		\item The behavior of such estimates in many samples of equal size is
		described by what are called \textbf{sampling distributions} \pause
		
		\item B\&M definition: The  sampling  distribution  of  a  statistic  is  the  distribution  of  values taken  by  the  statistic  in  all  possible  samples  of  the  same  size  from  the  same population.		
		
	\end{itemize}	
	
	
}

\frame{\frametitle{Why are sampling distributions important?} 
	
	\begin{itemize}
		\setlength\itemsep{2em} 
		\item They  tell  us  how  far  from  the  target  (true  value  of  the  parameter)  our  statistical  \emph{shot}  at  it  (i.e.  the  statistic  calculated  form  a  sample)  is  likely  to  be,  or, to  have  been.  \pause 
		
		\item Thus,  they  are  used  in  confidence  intervals  for  parameters. Specific  sampling  distributions  (based  on  a null value  for  the  parameter)  are also  used  in  statistical  tests  of  hypotheses.
		
	\end{itemize}	
	
	
}


\frame{\frametitle{Exercise 1: How Deep is the Ocean?} 
	
	\begin{itemize}
		\setlength\itemsep{2em} 
		\item We will get a sense of what a sampling distribution is in Exercise 1 \pause
		
		\item \textbf{CAVEAT}: This is a luxury using a toy example. In actual studies, we only get one shot!
		
	\end{itemize}	
	
	
}


\section{Exercise 1 Results}

\begin{frame}[fragile]{Sampling distribution: proportion covered by water}




\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=1\linewidth]{figure/unnamed-chunk-2-1} 

}



\end{knitrout}

\end{frame}



\begin{frame}[fragile]{Sampling distribution: proportion covered by water}




\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=1\linewidth]{figure/unnamed-chunk-4-1} 

}



\end{knitrout}

\end{frame}



\begin{frame}[fragile]{Sampling distribution: mean depth of the ocean}


\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=1\linewidth]{figure/unnamed-chunk-5-1} 

}



\end{knitrout}

\end{frame}



\begin{frame}[fragile]{Sampling distribution: mean depth of the ocean}


\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=1\linewidth]{figure/unnamed-chunk-6-1} 

}



\end{knitrout}

\end{frame}




%\begin{frame}[fragile]{Emulating the ``population''}




%\end{frame}



%\begin{frame}[fragile]{Emulating the ``population''}




%\end{frame}

\begin{frame}{Key takeaways and next steps}
\begin{enumerate}
	\setlength\itemsep{2em}
	\item We've been exclusively talking about point estimates \pause
	\item How confident are we about these point estimates? \pause
	\item \textcolor{blue}{Thought experiment}: Estimate the average temperature in Montreal in August over the past 100 years. \pause  
	\item We're going into stat territory now. 
\end{enumerate}
\end{frame}


\begin{comment}
\begin{frame}


The bigger the individual variaiton 

1) Normal curves. Show the 

2) the variance of y bar is the sum of the individual variances
standard error always have a root of n in the bottom

for now use 2.. estimate the sigma... Baldi and Moore give the sigma which makes no sense. 
what are the sample sizes..

start by asking 1+/- the SE, then +- 2 * SE, 
temperature in montreal for motivating the 100\%   confidence interval, 
temperature in august over the past 100 years, whos been here in montreal 0 years, 5
any one estimate has 0\% confidence


\end{frame}
\end{comment}


\section{Normal Curves and Calculations}


\subsection{Normal distribution}
\frame{\frametitle{The Normal (Gaussian) distribution} What is
	it?
	\begin{itemize}
		\item A distribution that describes continuous (numerical) data
		\item Can also be used to approximate discrete data distributions
		\item Range is (technically) infinite, though the probability of seeing
		very large or very small values is extremely tiny
		\item Fully described by only two parameters, the mean and variance
		($\mu$ and $\sigma^2$)
		\item \textcolor{red}{NOTE:} Baldi \& Moore (and \texttt{R}) use the short-hand: $X \sim
		\mathcal{N}(\mu,\sigma)$, denoting the normal distribution as a
		function of the mean and \textit{standard deviation}. This is not
		standard; many texts instead write $X \sim
		\mathcal{N}(\mu,\sigma^2)$. Be careful of this!
	\end{itemize}
} \frame{\frametitle{The Normal (Gaussian) distribution} Carl Gauss
	was a German mathematician who developed a number of important
	advances in statistics such as the method of least squares.
	\begin{figure}
		\begin{center}
			\epsfig{figure=Part2Figs/GaussMoney.eps,width=3.2in,height=2.5in}
		\end{center}
	\end{figure}
} 

\frame{\frametitle{The Normal distribution} Where do Normal data
	come from?
	\begin{itemize}
		\item Natural processes
		\begin{itemize}
			\item Blood pressure
			\item Height
			\item Weight
			\item[]
		\end{itemize} \pause
		\item ``Man-made'' (or derived)
		\begin{itemize}
			\item Binomial (proportion) and Poisson (count) data are
			approximately Normal under certain conditions
			\item Sums and means of random variables (Central Limit Theorem)
			\item Data can sometimes be made to look Normal via transformations
			(squares, logs, etc)
		\end{itemize}
	\end{itemize}
} 

\frame{\frametitle{The Normal distribution} For Normal data, we
	can use \sout{the Gaussian tables} \texttt{R} to answer the questions:
	\begin{itemize}
		\item What is the probability that a single observation $X$ is
		\begin{itemize}
			\item greater than $X^*$?
			\item less than $X^*$?
			\item between $X^*_L$ and $X^*_U$?
		\end{itemize}
		\item That is, we can find out information about the percent
		distribution of $X$ as a function of thresholds $X^*$, or $X^*_L$
		and $X^*_U$.
		\item[] \pause
		\item We can also use \sout{the Normal tables} \texttt{R} to find out information about
		thresholds $X^*$ that will contain particular percentages of
		the data. I.e., we can find what threshold values will
		\begin{itemize}
			\item Exclude the lower $\omega^*$\% of a population
			\item Exclude the upper $\omega^*$\% of a population
			\item Contain the middle $\omega^*$\% of a population
		\end{itemize}
	\end{itemize}
}

\frame{\frametitle{The Normal distribution} We can use \sout{the
	Gaussian tables} \texttt{R} to answer these questions \textbf{no matter
		what the values
		of} $\mu$ and $\sigma^2$. \\ \ \\
	That is, the \% of the Normal distribution falling between $X^*_L =
	\mu - m_1\sigma$ and $X^*_U = \mu + m_2\sigma$ where
	$m_1, m_2$ are any multiples \textbf{remains the same} for any $\mu$ and $\sigma$. \\ \ \\
	How so?? \pause
	\begin{center}
		Because we can \textbf{standardize} any $X \sim
		\mathcal{N}(\mu,\sigma)$ to find $Z \sim \mathcal{N}(0,1)$
\end{center} }

\frame{\frametitle{The Normal distribution} An illustration
	using IQ scores, which we presume have a $\mathcal{N}(100,13)$
	distribution of scores. \\ \ \\
	
	\textcolor{blue}{Q1:} What percentage of scores are
	\textbf{above}
	130? \\
	Two steps:
	\begin{enumerate}
		\item Change of location from $\mu_X=100$ to $\mu_Z=0$
		\item Change of scale from $\sigma_X=13$ to $\sigma_Z=1$
		\item[]
	\end{enumerate}
	Together, this gives us \[Z = \frac{X-\mu_X}{\sigma_X} =
	\frac{130-100}{13} = 2.31\] }

\begin{frame}[fragile]{The Normal distribution}

\vspace*{-.01in}

\small{The position of $X$=130 in a $\mathcal{N}(100,13)$ distribution is the same as
the place of $Z=2.31$ on the $\mathcal{N}(0,1)$, which we call the \textbf{standardized} Normal distribution (or	$Z$-distribution).}
	
	%\begin{figure}
%		\begin{center}
%			\epsfig{figure=Part1Figs/TwoNormals-equalProb.eps,width=4.4in,height=2.5in}
%		\end{center}
%	\end{figure}

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.55\linewidth]{figure/probs-1} 

}




{\centering \includegraphics[width=0.55\linewidth]{figure/probs-2} 

}



\end{knitrout}

\end{frame} 

\frame{\frametitle{The Normal distribution} How are the
	values
	in the Normal tables found?\\ \ \\
	Normal density:
	\[f(x|\mu,\sigma) = \frac{1}{\sqrt{2\pi}\sigma}\exp\frac{-(x-\mu)^2}{2\sigma^2}\]
	
	\vspace{.5cm} Probabilities found by integration (area under the
	Normal curve): \[ P(a \le x \le b) =
	\int_a^b\frac{1}{\sqrt{2\pi}\sigma}\exp\frac{-(x-\mu)^2}{2\sigma^2}dx\]
}


\frame{\frametitle{The Normal distribution} (The percent above
	$X=130$) = (\% above $Z=2.31$) =1.04\%\\ \ \\
	
	How do we know this? We look at the lower tail probability of
	2.31 [i.e., the \% below 2.31], and then subtract it from 1:
	\begin{enumerate}
		\item $P(X < 130) = P(Z < 2.31) = 0.9896$
		\item $P(X > 130) = 1 - P(X < 130) = 0.0104$
		\item[]
	\end{enumerate}
	So 130 is the 98.96$^{th}$ percentile of a $\mathcal{N}$(100,13)
	distribution. }


\begin{frame}{Reminder about percentiles and quantiles}

\begin{itemize}
	\item \textbf{Quantile}
	\begin{itemize}
		\item Any set of data, arranged in ascending or descending order, can be divided into various parts, also known as partitions or subsets, regulated by quantiles. 
		\item Quantile is a generic term for those values that divide the set into partitions of size $n$, so that each part represents $1/n$ of the set. 
		\item Quantiles are not the partition itself. They are the numbers that define the partition. 
		\item You can think of them as a sort of numeric boundary.
	\end{itemize}
\end{itemize}
\pause 

\begin{itemize}
	\item \textbf{Percentile}
	\begin{itemize}
		\item Percentiles are quite similar to quantiles: they split your set, but only into two partitions. 
		\item For a generic $k$th percentile, the lower partition contains k\% of the data, and the upper partition contains the rest of the data, which amounts to 100 - k \%, because the total amount of data is 100\%. 
		\item Of course k can be any number between 0 and 100.
	\end{itemize}
\end{itemize}

\end{frame}



\begin{frame}{More about percentiles and quantiles}
\begin{itemize}
	\item In class, we will find ourselves asking for the quantiles of a distribution. 
	\item Percentiles go from 0 to 100
	\item Quantiles go from any number to any number
	\item Percentiles are examples of quantiles and you might find some people use them interchangeably (though this may not always be correct since quantiles can take on any value, positive or negative). 
	\item \textbf{In particular}, \texttt{R} uses the term quantiles. 
	%\item This is a small semantic quibble, but we ought to be precise. 
	%\item That being said, I won't correct somebody if they call these percentiles. 
	\item \textcolor{blue}{In the previous example}, we saw that $P(Z < 2.31) = 0.9896$. In \texttt{R}, 2.31 is called the quantile 
	.
\end{itemize}
\end{frame}


\frame{\frametitle{The Normal distribution} (The percent above
	$X=130$) = (\% above $Z=2.31$) =1.04\%\\ \ \\
	
	But wait!! The standard Normal is symmetric about 0, so we can do
	this another way... The \% \textbf{above} 2.31 is equal to the \%
	\textbf{below} -2.31:
	\begin{itemize}
		\item[] $P(X > 130) = P(Z > 2.31)$ \pause
		\item[] $\qquad \Rightarrow$ $P(Z > 2.31) = P(Z < -2.31) $ \pause
		\item[] $\qquad \Rightarrow$ $P(X > 130) = P(Z < -2.31) = 0.0104$
		\item[]
	\end{itemize}
	So 130 is the 98.96$^{th}$ percentile of a $\mathcal{N}(130,13)$ distribution. What is
	the 1.04$^{th}$ percentile? \\ \ \\\pause
	
	Transform from $Z = -2.31$ back to $X$:
	\[ X = \sigma Z + \mu = 13(-2.31) + 100 = 69.97.\]
	
}

\begin{frame}[fragile]{For probabilities we use $pnorm$}


\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\begin{alltt}
\hlstd{stats}\hlopt{::}\hlkwd{pnorm}\hlstd{(}\hlkwc{q} \hlstd{=} \hlnum{130}\hlstd{,} \hlkwc{mean} \hlstd{=} \hlnum{100}\hlstd{,} \hlkwc{sd} \hlstd{=} \hlnum{13}\hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] 0.9894919
\end{verbatim}

\end{knitrout}

\pause 

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\begin{alltt}
\hlstd{mosaic}\hlopt{::}\hlkwd{xpnorm}\hlstd{(}\hlkwc{q} \hlstd{=} \hlnum{130}\hlstd{,} \hlkwc{mean} \hlstd{=} \hlnum{100}\hlstd{,} \hlkwc{sd} \hlstd{=} \hlnum{13}\hlstd{)}
\end{alltt}


{\centering \includegraphics[width=0.6\linewidth]{figure/probs3-1} 

}


\begin{verbatim}
## [1] 0.9894919
\end{verbatim}

\end{knitrout}

\pause 

\begin{itemize}
	\item \texttt{pnorm} returns the integral from $-\infty$ to $q$ for a $\mathcal{N}(\mu, \sigma)$
	\item \texttt{pnorm} goes from \textit{quantiles} (think $Z$ scores) to probabilities
\end{itemize}

\end{frame}



\begin{frame}[fragile]{For quantiles we use $qnorm$}



\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\begin{alltt}
\hlstd{stats}\hlopt{::}\hlkwd{qnorm}\hlstd{(}\hlkwc{p} \hlstd{=} \hlnum{0.0104}\hlstd{,} \hlkwc{mean} \hlstd{=} \hlnum{100}\hlstd{,} \hlkwc{sd} \hlstd{=} \hlnum{13}\hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] 69.94926
\end{verbatim}

\end{knitrout}

\pause 

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\begin{alltt}
\hlstd{mosaic}\hlopt{::}\hlkwd{xqnorm}\hlstd{(}\hlkwc{p} \hlstd{=} \hlnum{0.0104}\hlstd{,} \hlkwc{mean} \hlstd{=} \hlnum{100}\hlstd{,} \hlkwc{sd} \hlstd{=} \hlnum{13}\hlstd{)}
\end{alltt}


{\centering \includegraphics[width=0.6\linewidth]{figure/probs5-1} 

}


\begin{verbatim}
## [1] 69.94926
\end{verbatim}

\end{knitrout}

\pause 

\small{
\begin{itemize}
	\item \texttt{qnorm} answers the question: What is the Z-score of the $p$th percentile of the normal distribution?
		
	\item \texttt{qnorm} goes from \textit{probabilities} to quantiles 
\end{itemize}
}
\end{frame}


\frame{\frametitle{The Normal distribution}
	\textcolor{blue}{Q2:} What is the probability of seeing an IQ
	score \textbf{as extreme as} (think highly unusual)  130? \\
	\begin{enumerate}
		\item Again, we find that $X=130$ is the same percentile of the
		IQ Normal distribution as $Z=2.31$ is of the standard Normal. \pause
		\item To see what scores are as extreme, we want to know the
		probability that $Z>$2.31 or that $Z<$-2.31. \pause
		\item As we saw previously, $P(Z > 2.31) = P(Z < -2.31) =
		0.0104$, so the probability of seeing an IQ as extreme or more
		so than 130 is $2\times0.0104 = 0.0208$.
	\end{enumerate}
}


\begin{frame}[fragile]{Finding tail probabilities}



\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\begin{alltt}
\hlcom{# lower.tail = TRUE is the default}
\hlstd{stats}\hlopt{::}\hlkwd{pnorm}\hlstd{(}\hlkwc{q} \hlstd{=} \hlopt{-}\hlnum{2.31}\hlstd{,} \hlkwc{mean} \hlstd{=} \hlnum{0}\hlstd{,} \hlkwc{sd} \hlstd{=} \hlnum{1}\hlstd{,} \hlkwc{lower.tail} \hlstd{=} \hlnum{TRUE}\hlstd{)} \hlopt{+}
\hlstd{stats}\hlopt{::}\hlkwd{pnorm}\hlstd{(}\hlkwc{q} \hlstd{=} \hlnum{2.31}\hlstd{,} \hlkwc{mean} \hlstd{=} \hlnum{0}\hlstd{,} \hlkwc{sd} \hlstd{=} \hlnum{1}\hlstd{,} \hlkwc{lower.tail} \hlstd{=} \hlnum{FALSE}\hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] 0.02088815
\end{verbatim}

\end{knitrout}

\pause 

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\begin{alltt}
\hlstd{mosaic}\hlopt{::}\hlkwd{xpnorm}\hlstd{(}\hlkwc{q} \hlstd{=} \hlkwd{c}\hlstd{(}\hlopt{-}\hlnum{2.31}\hlstd{,}\hlnum{2.31}\hlstd{),} \hlkwc{mean} \hlstd{=} \hlnum{0}\hlstd{,} \hlkwc{sd} \hlstd{=} \hlnum{1}\hlstd{)}
\end{alltt}


{\centering \includegraphics[width=0.6\linewidth]{figure/probs7-1} 

}


\begin{verbatim}
## [1] 0.01044408 0.98955592
\end{verbatim}

\end{knitrout}


\end{frame}



\begin{frame}[fragile]{The Normal distribution}
	\textcolor{blue}{Q3:}
	What is the 75$^{th}$ percentile of the IQ scores distribution? \\
	We now have to reverse the sequence of steps:
	\begin{itemize}
		\item \textcolor{blue}{Ask yourself:} What $Z$ value corresponds to a probability of 0.75? Should you use \texttt{pnorm} or \texttt{qnorm}? \pause

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\begin{alltt}
\hlstd{mosaic}\hlopt{::}\hlkwd{xqnorm}\hlstd{(}\hlkwc{p} \hlstd{=} \hlnum{0.75}\hlstd{,} \hlkwc{mean} \hlstd{=} \hlnum{100}\hlstd{,} \hlkwc{sd} \hlstd{=} \hlnum{13}\hlstd{)}
\end{alltt}


{\centering \includegraphics[width=0.6\linewidth]{figure/probs8-1} 

}


\begin{verbatim}
## [1] 108.7684
\end{verbatim}

\end{knitrout}

		\item[]
	\end{itemize} This gives us that 75\% of the IQ scores fall below 108.8. 
\end{frame}


\begin{frame}[fragile]{Empirical Rule or 68-95-99.7\% Rule}

In any normal distribution with mean $\mu$ and standard deviation $\sigma^2$:
\begin{itemize}
			\setlength\itemsep{2em}
	\item Approximately 68\% of the data fall within one standard deviation of the mean.
\item Approximately 95\% of the data fall within two standard deviations of the mean.
\item Approximately 99.7\% of the data fall within three standard deviations of the mean.
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Demo of Empirical Rule}

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\begin{alltt}
\hlstd{pacman}\hlopt{::}\hlkwd{p_load}\hlstd{(mosaic)}
\hlstd{pacman}\hlopt{::}\hlkwd{p_load}\hlstd{(manipulate)}

\hlstd{mNorm} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{mean} \hlstd{=} \hlnum{0}\hlstd{,} \hlkwc{sd} \hlstd{=} \hlnum{1}\hlstd{) \{}
  \hlstd{lo} \hlkwb{<-} \hlstd{mean} \hlopt{-} \hlnum{5} \hlopt{*} \hlstd{sd}
  \hlstd{hi} \hlkwb{<-} \hlstd{mean} \hlopt{+} \hlnum{5} \hlopt{*} \hlstd{sd}
  \hlkwd{manipulate}\hlstd{(}
        \hlkwd{xpnorm}\hlstd{(}\hlkwd{c}\hlstd{(A,B), mean, sd,} \hlkwc{verbose} \hlstd{=} \hlnum{FALSE}\hlstd{,} \hlkwc{invisible} \hlstd{=} \hlnum{TRUE}\hlstd{),}
        \hlkwc{A} \hlstd{=} \hlkwd{slider}\hlstd{(lo, hi,} \hlkwc{initial} \hlstd{= mean} \hlopt{-} \hlstd{sd),}
        \hlkwc{B} \hlstd{=} \hlkwd{slider}\hlstd{(lo, hi,} \hlkwc{initial} \hlstd{= mean} \hlopt{+} \hlstd{sd)}
 \hlstd{)}
\hlstd{\}}
\hlkwd{mNorm}\hlstd{(}\hlkwc{mean} \hlstd{=} \hlnum{0}\hlstd{,} \hlkwc{sd} \hlstd{=} \hlnum{1}\hlstd{)}
\end{alltt}

\end{knitrout}
\end{frame}


\begin{frame}[fragile]{Empirical Rule or 68-95-99.7\% Rule}

\framedgraphic{6899rule.png}

\end{frame}

\frame{\frametitle{Properties of Normal random variables} Special
	properties of the Normal distribution:
	\begin{itemize}
		\item[] \item In $X$ is a Normal random variables, then so is $a+bX$. \pause
		\item[] \item If $X$ and $Y$ are two Normal random variables, then
		$X+Y$ is a Normal random variable. What is the mean and variance of this new random
		variable? \pause
		\item[] \item If $X$ and $Y$ are two Normal random variables and
		$\rho_{XY}=0$, then $X$ and $Y$ are independent.
	\end{itemize}
} \frame{\frametitle{Properties of Normal random variables} Example:
	Let $Y_1,...,Y_n \sim \mathcal{N}(\mu,\sigma)$, and let each $Y_i$
	be independent of the others.\\ \ \\
	
	Then $\overline{Y} = \frac{1}{n}\sum_{i=1}^n Y_i$ has what distribution? \pause
	\begin{itemize}
		\item The sum of Normal random variables is Normal, so $\overline{Y}$ is a
		Normal random variable. \pause
		\item $E(\overline{Y}) = \frac{1}{n}\sum_{i=1}^n E(Y_i) =  \frac{1}{n}\sum_{i=1}^n \mu =
		\mu$.
		\item $Var(\overline{Y}) = Var(\frac{1}{n}\sum_{i=1}^n Y_i) =  \frac{1}{n^2}\sum_{i=1}^n Var(Y_i) =
		\sigma^2/n$.
		\item Standard Error of $\overline{Y}$ = $\sqrt{Var(\overline{Y})} = \sigma / \sqrt{n}$
	\end{itemize}
}



\begin{comment}
\frame{\frametitle{Properties of estimators} As we have discussed,
	there are several properties that we want in an estimator:
	\begin{enumerate}
		\item Unbiased: the expected value (mean) of the estimator is equal to the
		parameter
		\begin{itemize}
			\item The sample mean, $\overline{x}$, is unbiased for the population
			mean, $\mu$
		\end{itemize} \pause
		\item Consistent: as sample size gets larger, the estimator gets
		closer to the parameter \pause
	\end{enumerate}
	How do we go about finding an estimator might have good properties?
	\\ \ \\ \pause
	
	One approach to finding good estimates is to ask ``What value of the parameter is most
	likely, given the data that I observed?'' Such an estimate is a \textbf{Maximum
		Likelihood Estimator} (MLE). } \frame{\frametitle{Maximum Likelihood Estimators} Suppose
	we have a sample $x_1,x_2,...,x_n$ from a Normally distributed population with unknown
	mean $\mu$ and known variance $\sigma^2=1$. We want to estimate $\mu$ from the data. The
	obvious estimator is the sample mean, $\bar{x}$. \\ \ \\\pause
	
	We have already shown that the sample mean is unbiased (that is,
	$E[\bar{x}] = \mu$) and that we can control its variability through
	the size of our sample (since Var$[\bar{x}] = \sigma^2/n$). Are
	there other theoretically justifications for using the sample mean?
} \frame{\frametitle{Maximum Likelihood Estimators} It turns out
	that $\bar{x}$ maximizes the probability of the observed data, given
	$\mu$. An estimator that has this property is a \textbf{maximum
		likelihood estimator}.
	\begin{figure}
		\begin{center}
			\epsfig{figure=Part2Figs/MLE-R-1.eps,width=3in,height=2.7in}
		\end{center}
	\end{figure}
} \frame{\frametitle{Maximum Likelihood Estimators} It turns out
	that $\bar{x}$ maximizes the probability of the observed data, given
	$\mu$. An estimator that has this property is a \textbf{maximum
		likelihood estimator}.
	\begin{figure}
		\begin{center}
			\epsfig{figure=Part2Figs/MLE-R-2.eps,width=3in,height=2.7in}
		\end{center}
	\end{figure}
} \frame{\frametitle{Maximum Likelihood Estimators} It turns out
	that $\bar{x}$ maximizes the probability of the observed data, given
	$\mu$. An estimator that has this property is a \textbf{maximum
		likelihood estimator}.
	\begin{figure}
		\begin{center}
			\epsfig{figure=Part2Figs/MLE-R-3.eps,width=3in,height=2.7in}
		\end{center}
	\end{figure}
} \frame{\frametitle{Maximum Likelihood Estimators} It turns out
	that $\bar{x}$ maximizes the probability of the observed data, given
	$\mu$. An estimator that has this property is a \textbf{maximum
		likelihood estimator}.
	\begin{figure}
		\begin{center}
			\epsfig{figure=Part2Figs/MLE-R-4.eps,width=3in,height=2.7in}
		\end{center}
	\end{figure}
}
content...
\end{comment}

\begin{comment}
\frame{\frametitle{Maximum Likelihood Estimators} For each data
point, the probability density function is given by:
\[ f(x_i|\mu,\sigma) = \frac{1}{\sqrt{2p}\sigma}\exp\left(-\frac{(x_i-\mu)^2}{2\sigma^2}\right). \]
Since the data are independent, we can use the product rule:
\begin{eqnarray*}
f(x_1,x_2,...,x_n|\mu,\sigma) & = &
\prod_{i=1}^n\frac{1}{\sqrt{2p}\sigma}\exp\left(-\frac{(x_i-\mu)^2}{2\sigma^2}\right)\\
& = & \frac{1}{\sqrt{2p}\sigma}\exp\left(-\frac{\sum_{i=1}^n(x_i-\mu)^2}{2\sigma^2}\right).\\
\end{eqnarray*}
This function is called a \textbf{likelihood function}; it describes
the likelihood (or the probability) of the observed data for a given
value of $\mu$ (and $\sigma$). }

\frame{\frametitle{Maximum Likelihood Estimators} We want to maximize this probability
function. That is, we want to find the value of $\mu$ that gives the highest probability
to the data we observed. \\ \ \\ \pause

From calculus, we know that we maximize a function by taking its derivative and set it
equal to zero. Doing this and simplifying gives
\[\frac{1}{2\sigma^2}\sum_{i=1}^n2(x_i-\mu) = 0.\] \pause

This implies \[\sum_{i=1}^n(x_i-\mu) = \sum_{i=1}^nx_i - n\mu  = 0\]
so that the likelihood is maximized at $\mu=
\frac{1}{n}\sum_{i=1}^nx_i = \bar{x}.$ Thus the MLE of $\mu$ is
$\bar{x}.$ }
\end{comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{comment}
\subsection{Normal approximation to binomial}

\frame{\frametitle{Normal approximation of the binomial
		distribution} Suppose we take a large random sample of size $n$
	in our population, and count $X$, the number of `successes'
	(success could be the event of having MS, the event of
	exercising more than twice per week, etc.).
	\begin{itemize}
		\item We can view $X$ as being sampled from a Binomial$(n,p)$
		distribution... \pause
		\item OR we can view $X$ as the sum of $n$ binary variable
		$Y_1$,...,$Y_n$ where $Y_i \sim$ Binomial$(1,p)$.
		\item The Binomial$(1,p)$ distribution is also called a
		Bernoulli$(p)$ distribution.
	\end{itemize}
} \frame{\frametitle{Normal approximation of the binomial} Now
	$\hat{p} = X/n$ is the sample proportion of successes, which is an
	estimate of $p$, the mean of the Bernoulli$(p)$ distribution. \\ \ \\
	
	If $\min(np,n(1-p)) \ge 10$, then we have
	\begin{itemize}
		\item $X$ is approximately distributed $\mathcal{N}(np,\sqrt{np(1-p)})$
		\item $\hat{p}$ is approximately distributed $\mathcal{N}(\quad,\quad\quad)$
	\end{itemize}
} \frame{\frametitle{Normal approximation of the binomial}
	Why is this true? \\
	\begin{itemize}
		\item[] \item For $\hat{p}$, this is easy to explain: $\hat{p}$ is just a
		sample average of Bernoulli random variables, since \[\hat{p} = X/n =
		(\frac{1}{n}\sum_{i=1}^n Y_i),\] and so the CLT applies. \pause
		\item[] \item But what about $X$? $X$ is just a linear
		transformation of $\hat{p}$, since $X = n\hat{p}$, and linear
		combinations of (approximately) Normal random variables are
		(approximately) Normal.
		\item[] \item The approximation is best when $p$ is near 0.5 (which makes the
		distribution of $X$ symmetric) and/or $n$ is large.
	\end{itemize}
} \frame{\frametitle{Normal approximation of the binomial}
	\begin{figure}
		\begin{center}
			\epsfig{figure=Part2Figs/NormalApproxBino.eps,width=3.3in,height=3.3in}
		\end{center}
	\end{figure}
}

\frame{\frametitle{Normal approximation of the binomial}
	Suppose we collect a simple random sample of $n=150$ health
	care workers in India. What is the probability that $X=80$ or
	more of the individuals show antibody response to TB
	(indicating previous TB exposure) if $p=0.6$? \\ \ \\
	
	Our binomial tables do not cover the cases for $n$ as large as
	150. Also, calculating
	\[ P(X = 80\hbox{ out of } 150) =
	\frac{150!}{(150-80)!80!}0.6^{80}0.4^{(150-80)}\] is difficult at
	best. 150! is a number that is over 250 digits long, and $0.6^{80}$
	is very, very small. Not only that, but to find $P(X \ge 80\hbox{ out of } 150)$, we would have to sum 70
	terms to calculate this probability. }

\frame{\frametitle{Normal approximation of the binomial} Let's
	use the Normal approximation!! \\ \ \\
	First, we need to calculate the mean and variance of the
	distribution:
	\begin{eqnarray*}
		\mu & = & np = 150\times0.6 = 90\\
		\sigma^2 & = & np(1-p) = 150\times0.6\times0.4 = 36
	\end{eqnarray*} \pause
	The Normal approximation to the binomial is improved by using a
	\textbf{continuity correction}, so that instead of estimation $P(X
	\ge 80|X\sim\hbox{Bino}(150,0.6))$ we estimate $P(X \ge
	79.5|X\sim\hbox{Bino}(150,0.6))$, and approximate this by $P(X \ge
	79.5|X\sim\mathcal{N}(90,6))$. }

\frame{\frametitle{Normal approximation of the binomial}
	{\small
		\[P(X \ge 80|X\sim\hbox{Bino}(150,0.6)) = P(X \ge
		79.5|X\sim\hbox{Bino}(150,0.6))\]
		\begin{eqnarray*}
			\qquad \qquad & \approx & P(X \ge 79.5|X\sim\mathcal{N}(90,6))\\
			\pause
			& = & P\left(\frac{X-\mu}{\sigma} \ge
			\frac{79.5-\mu}{\sigma}|X\sim\mathcal{N}(90,6)\right)\\ \pause
			& = & P\left(\frac{X-90}{6} \ge
			\frac{79.5-90}{6}|X\sim\mathcal{N}(90,6)\right)\\ \pause
			& = & P\left(Z \ge -1.75|Z\sim\mathcal{N}(0,1)\right)\\
			& = & 0.9599
	\end{eqnarray*} }
}

\frame{\frametitle{Normal approximation of the binomial} What
	difference does the continuity correction make? \\ \ \\ {\small
		\[P(X \ge 80|X\sim\hbox{Bino}(150,0.6)) \qquad \qquad \qquad \qquad \qquad \qquad \]
		\begin{eqnarray*}
			\qquad \qquad & \approx & P\left(\frac{X-\mu}{\sigma} \ge
			\frac{80-\mu}{\sigma}|X\sim\mathcal{N}(90,6)\right)\\
			& = & P\left(\frac{X-90}{6} \ge
			\frac{80-90}{6}|X\sim\mathcal{N}(90,6)\right)\\
			& = & P\left(Z \ge -1.667|Z\sim\mathcal{N}(0,1)\right)\\
			& = & 0.9522
	\end{eqnarray*} }
	...In this case, it doesn't matter much. BUT...
}

\frame{\frametitle{Normal approximation of the binomial} Now we
	ask the question, ``What is the probability that exactly $X=80$
	of the health care workers show antibody response to TB?'' \\ \ \\
	
	If we \textit{don't} use the continuity correction, we find:
	{\small
		\[P(X = 80|X\sim\hbox{Bino}(150,0.6)) \qquad \qquad \qquad \qquad \qquad \qquad \]
		\begin{eqnarray*}
			\qquad \qquad & \approx & P\left(\frac{X-\mu}{\sigma} = \frac{80-\mu}{\sigma}|X\sim\mathcal{N}(90,6)\right)\\
			& = & P\left(\frac{X-90}{6} = \frac{80-90}{6}|X\sim\mathcal{N}(90,6)\right)\\
			& = & P\left(Z= -1.667|Z\sim\mathcal{N}(0,1)\right)\\
			& = & 0
	\end{eqnarray*}  }
	(Why this this probability zero?)
}

\frame{\frametitle{Normal approximation of the binomial} Using
	the continuity correction gives {\small
		\[P(X = 80|X\sim\hbox{Bino}(150,0.6)) \qquad \qquad \qquad \qquad \qquad \qquad \]
		\begin{eqnarray*}
			\qquad \qquad & = & P(79.5 \le X \le
			80.5|X\sim\hbox{Bino}(150,0.6))\\
			& \approx & P(79.5 \le X \le 80.5|X\sim\mathcal{N}(90,6))\\
			& = & P\left(\frac{79.5-\mu}{\sigma} \le \frac{X-\mu}{\sigma} \le
			\frac{80.5-\mu}{\sigma}|X\sim\mathcal{N}(90,6)\right)\\
			& = & P\left(\frac{79.5-90}{6}| \le \frac{X-90}{6} \le
			\frac{80.5-90}{6}|X\sim\mathcal{N}(90,6)\right)\\
			& = & P\left(-1.75 < Z \le -1.583|Z\sim\mathcal{N}(0,1)\right)\\
			& = & 0.0166
	\end{eqnarray*}} \pause
	
	Using the binomial distribution formula, the exact answer is
	0.01659816. The Normal approximation with the continuity
	correction is correct to 4 decimal places.}
\end{comment}

\section{Central Limit Theorem}


\frame{\frametitle{Properties of the sample mean: The Central Limit Theorem (CLT)} The
	sampling distribution of $\overline{X}$ is Normal if $X$ is Normal. What probability
	distribution does the sample mean follow if $X$ is not Normal?\\ \ \\ \pause
	
	\textcolor{blue}{As sample size increases, the distribution of
		$\overline{X}$ becomes closer to a Normal distribution, no matter
		what the distribution of sampled variable $X$!} \\ \ \\
	(This is true as long as the distribution has a finite variance.) }

\frame{\frametitle{The Central Limit Theorem (CLT)} The Central
	Limit Theorem (CLT) tells us that, no matter the distribution of
	$X$, if $\sigma^2$ is finite, \[\overline{X} \sim
	\mathcal{N}(\mu,\sigma/\sqrt{n}).\]
	
	\vspace{1.25cm}
	%pause
	Recall: When we are talking about the variability of a
	\textbf{statistic}, we use the term \textbf{standard error} (not
	standard deviation). The standard error of the sample mean is
	$\sigma/\sqrt{n}$. }

\frame{\frametitle{The Central Limit Theorem (CLT)}
	\begin{figure}
		\begin{center}
			\epsfig{figure=Part2Figs/CLT1.eps,width=3.3in,height=3.3in}
		\end{center}
	\end{figure}
} \frame{\frametitle{The Central Limit Theorem (CLT)}
	\begin{figure}
		\begin{center}
			\epsfig{figure=Part2Figs/CLT2.eps,width=3.3in,height=3.3in}
		\end{center}
	\end{figure}
} \frame{\frametitle{The Central Limit Theorem (CLT)}
	\begin{figure}
		\begin{center}
			\epsfig{figure=Part2Figs/CLT3.eps,width=3.3in,height=3.3in}
		\end{center}
	\end{figure}
}






%\end{frame}

\begin{frame}[fragile]{CLT in action: Depths of the ocean}
\includegraphics<1>[width=\textwidth,height=0.8\textheight,keepaspectratio]{ocean1.png}
\includegraphics<2>[width=\textwidth,height=0.8\textheight,keepaspectratio]{ocean2.png}
\includegraphics<3>[width=\textwidth,height=0.8\textheight,keepaspectratio]{ocean3.png}
\includegraphics<4>[width=\textwidth,height=0.8\textheight,keepaspectratio]{ocean4.png}
\includegraphics<5>[width=\textwidth,height=0.8\textheight,keepaspectratio]{ocean5.png}
\includegraphics<6>[width=\textwidth,height=0.8\textheight,keepaspectratio]{ocean6.png}
\includegraphics<7>[width=\textwidth,height=0.8\textheight,keepaspectratio]{ocean7.png}
\includegraphics<8>[width=\textwidth,height=0.8\textheight,keepaspectratio]{ocean8.png}
\includegraphics<9>[width=\textwidth,height=0.8\textheight,keepaspectratio]{ocean9.png}
\includegraphics<10>[width=\textwidth,height=0.8\textheight,keepaspectratio]{ocean10.png}
\includegraphics<11>[width=\textwidth,height=0.8\textheight,keepaspectratio]{ocean17.png}
\includegraphics<12>[width=\textwidth,height=0.8\textheight,keepaspectratio]{oceanAll.png}
%\includegraphics<3>{C}
\end{frame}

\begin{frame}[fragile]{Quadruple the work, half the benefit}

\framedgraphiccaption{ROOToceanAll.png}{When the sample size increases from 4 to 16, the spread of the sampling distribution for the mean is reduced by a half, i.e., the range is cut in half. This is known as the curse of the $\sqrt{n}$}
\end{frame}









\end{document}



\begin{frame}[fragile]{Collecting data takes effort}

\begin{comment}

\Wider[9em]{
	\begin{figure}
		\begin{minipage}[h]{0.49\linewidth}
			\centering
			\includegraphics[scale=0.4]{cart_pruned-crop.pdf}
			\caption{Regression}
			\label{fig:a}
		\end{minipage}
		%\hspace{0.5cm}
		\begin{minipage}[h]{0.49\linewidth}
			\centering
			\includegraphics[scale=0.20]{class_tree-crop.pdf}
			\caption{Classification}
			\label{fig:b}
		\end{minipage}
	\end{figure}
}
\end{comment}


\begin{itemize}
	\item Today's class $\to$ regression
\end{itemize}

\end{frame}







\begin{frame}{How does CART work?}
	
	Roughly speaking, there are two steps~\cite{james2013introduction}:
	\pause 
	
	\begin{enumerate}
		\setlength\itemsep{1em}
		%\item Un arbre de régression et de classification se construit de manière itérative, en découpant à chaque étape la population en deux sousensembles\nocite{lopez2015arbres}
		%\item Le découpage s’effectue suivant des règles simples portant sur les variables	explicatives, en déterminant la règle optimalequi permet de construire deux populations les	plus différenciées en termes de valeurs de la variable à expliquer
		\item We divide the predictor space - that is, the set of possible values for
		$X_1, X_2,\ldots, X_p$, into $J$ non-overlapping and exhaustive regions,
		$R_1, R_2,\ldots, R_J$.
		\item For every observation that falls into the region $R_j$, we make the same
		prediction, which is simply the mean of the response values for the
		training observations in $R_j$.
	\end{enumerate}
\end{frame}







\begin{frame}{And if we continue...}

gdsgs

%\framedgraphic{area5.pdf}

\end{frame}

\begin{frame}{Stop if the number of observations is less than 20}

gsd

%\Wider[4em]{
%\includegraphics[scale=0.45]{tree5.pdf}
%}

\end{frame}


\section{The Details}


\begin{frame}{The Details}
	
The CART algorithm requires 3 components:

\begin{enumerate}
		  \setlength\itemsep{1.5em}
	\item Defining a criterion to select the best partition among all predictors.
	\item A rule to decide when a node is terminal, i.e., it becomes a leaf.
	\item Pruning the tree to avoid over-fitting.
\end{enumerate}

\end{frame}


\begin{frame}{1. Selecting the Best Partition}
	The objective is the find the regions $R_1, \ldots, R_J$ that minimize the squared error loss:
		
	\begin{equation}
		\sum_{j=1}^{J} \sum_{i \in R_j}(y_i - \hat{y}_{R_j})^2 \label{eq:obj1}
	\end{equation}
	
	\begin{itemize}
		\setlength\itemsep{1.5em}
		\item $\hat{y}_{R_j}$: the mean response for the training observations within the
		$j$th box \pause
		\item Finding the solution to~\eqref{eq:obj1} is computationally infeasible (\textit{NP-hard}). Why? 
	\end{itemize}
	
	
\end{frame}




\section{Comparison with a Linear Model}

\begin{frame}{Comparison: Linear Model vs. CART}
\vspace*{-0.25cm}

\ctable[pos=h!,doinside=\footnotesize]{lcc}{\tnote{\cmark: yes, \xmark: no}
}{
\FL
\textbf{Characteristic\tmark} & \textbf{\texttt{Linear Model}}   & \textbf{\texttt{CART}} \ML
\rowcolor{whitesmoke}
Linearity Assumption & \cmark &    \xmark    \\
& &  \\
Distributional Assumptions & \cmark & \xmark \\
& &  \\
\rowcolor{whitesmoke}
Robust to multicollinearity &  \xmark  &  \cmark   \\ 
%& &  \\
%High-dimensional data ($n << p$) & \xmark   & \cmark     \\
& &  \\
\rowcolor{whitesmoke}
Handles complex interactions &  \xmark  &  \cmark    \\
& &  \\
%Valeurs aberrantes  &    &      \\
\rowcolor{whitesmoke} 
Allows for missing data  & \xmark   &  \cmark    \\
& &  \\
Confidence Intervals, $p$-values & \cmark & \xmark \LL
}
\end{frame}




\begin{frame}[fragile]{Linear Model}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\begin{alltt}
\hlkwd{lm}\hlstd{(Salary} \hlopt{~} \hlstd{Years} \hlopt{*} \hlstd{Hits,} \hlkwc{data} \hlstd{= Hitters)}
\end{alltt}

\end{knitrout}


\end{frame}





\begin{frame}[allowframebreaks]
%\nocite{breiman1984classification}
	\nocite{friedman2001elements}
	\nocite{james2013introduction}
	\nocite{lopez2015arbres}
	\frametitle{References}
\printbibliography
\end{frame}


\begin{frame}[fragile]{Session Info}
	\tiny
	
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\begin{verbatim}
R version 3.5.1 (2018-07-02)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Ubuntu 18.04 LTS

Matrix products: default
BLAS: /usr/lib/x86_64-linux-gnu/openblas/libblas.so.3
LAPACK: /usr/lib/x86_64-linux-gnu/libopenblasp-r0.2.20.so

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
 [1] tidyr_0.8.1        mosaic_1.4.0       Matrix_1.2-14     
 [4] mosaicData_0.17.0  ggformula_0.9.0    ggstance_0.3.1    
 [7] ggplot2_3.0.0.9000 lattice_0.20-35    dplyr_0.7.6       
[10] knitr_1.20        

loaded via a namespace (and not attached):
 [1] Rcpp_0.12.18      pillar_1.3.0      compiler_3.5.1   
 [4] highr_0.7         plyr_1.8.4        bindr_0.1.1      
 [7] tools_3.5.1       digest_0.6.16     viridisLite_0.3.0
[10] nlme_3.1-137      evaluate_0.11     tibble_1.4.2     
[13] gtable_0.2.0      pkgconfig_2.0.2   rlang_0.2.2      
[16] ggrepel_0.8.0     ggdendro_0.1-20   bindrcpp_0.2.2   
[19] gridExtra_2.3     withr_2.1.2       stringr_1.3.1    
[22] grid_3.5.1        tidyselect_0.2.4  mosaicCore_0.6.0 
[25] glue_1.3.0        R6_2.2.2          pacman_0.4.6     
[28] purrr_0.2.5       magrittr_1.5      codetools_0.2-15 
[31] backports_1.1.2   splines_3.5.1     scales_1.0.0     
[34] MASS_7.3-50       assertthat_0.2.0  colorspace_1.4-0 
[37] labeling_0.3      stringi_1.2.4     lazyeval_0.2.1   
[40] munsell_0.5.0     broom_0.5.0       crayon_1.3.4     
\end{verbatim}

\end{knitrout}

\end{frame}

\end{document}
