\documentclass[handout]{beamer}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}

\usepackage{default}
\usepackage{animate} %need the animate.sty file 
\usepackage{graphicx}
%\graphicspath{{/home/sahir/Dropbox/jobs/laval/minicours/slides/}}
\usepackage{hyperref, url}
%\usepackage[round,sort]{natbib}   % bibliography omit 'round' option if you prefer square brackets
%\bibliographystyle{apalike}
\usepackage{biblatex}
\bibliography{bib.bib}
% Removes icon in bibliography
\setbeamertemplate{bibliography item}[text]

\usepackage[normalem]{ulem}

\setbeamertemplate{theorems}[numbered]



%\newtheorem{prop}{Proposition}
%\newenvironment{theoremc}[1]
%{\begin{shaded}\begin{theorem}[#1]}
%		{\end{theorem}\end{shaded}}
	
%\newtheorem{examplefirst}{Example}
%\newtheorem{examplesecond}{Example}
%\newenvironment<>{examplefirst}[1][]{%
%	\setbeamercolor{block title example}{bg=lightgray}%
%	\begin{example}#2[#1]}{\end{example}}
%\newenvironment<>{examplesecond}[1][]{%
%	\setbeamercolor{block title example}{fg=white,bg=blue!75!black}%
%	\begin{example}#2[#1]}{\end{example}}	

%\usepackage{amsthm}


\usepackage[figurename=Fig.]{caption}
\usepackage{subfig}
\usepackage{tikz, pgfplots,epsfig}
\usetikzlibrary{arrows,shapes.geometric}
\usepackage{color, colortbl,xcolor}
\definecolor{lightgray}{RGB}{200,200,200}
\definecolor{palegray}{RGB}{221,221,221}
\definecolor{myblue}{RGB}{0,89,179}
\usepackage{comment}
\setbeamercolor{frametitle}{fg=myblue}
\setbeamercolor{section in head/foot}{bg=myblue, fg=white}
\setbeamercolor{author in head/foot}{bg=myblue}
\setbeamercolor{date in head/foot}{bg=myblue}

\usepackage{shadethm}
%\colorlet{shadecolor}{blue!15}
\colorlet{shadecolor}{palegray}
%\setlength{\shadeboxrule}{.4pt}

\newshadetheorem{thm}{Theorem}
\newshadetheorem{defm}{Definition}
\newshadetheorem{exm}{Exercise}
\newshadetheorem{remarkm}{Remark}
%\definecolor{shadethmcolor}{HTML}{EDF8FF}
\definecolor{shadethmcolor}{RGB}{221,221,221}
%\definecolor{shaderulecolor}{HTML}{45CFFF}
\definecolor{shaderulecolor}{RGB}{0,89,179}
\setlength{\shadeboxrule}{.4pt}


\usepackage{array}
\newcolumntype{L}{>{\centering\arraybackslash}m{3cm}} % used for text wrapping in ctable
\usepackage{ctable}
\usepackage[utf8]{inputenc}
\usepackage{fontenc}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\def\widebar#1{\overline{#1}}
\definecolor{whitesmoke}{rgb}{0.96, 0.96, 0.96}

\usepackage{amssymb}
\usepackage{amsmath}

\usepackage{bm}
\def\transpose{{\sf{T}}}
\def\E{{\skew0\bm{E}}}
\def\Xvec{{\skew0\bm{X}}}
\def\Xveca{{\skew0\bm{X}}_1}
\def\Xvecb{{\skew0\bm{X}}_2}

\def\Yvec{{\skew0\bm{Y}}}
\def\bmY{{\skew0\bm{Y}}}
\def\bmX{{\skew0\bm{X}}}
\def\bmy{{\skew0\bm{y}}}
\def\bmG{{\skew0\bm{G}}}
\def\bmS{{\skew0\bm{S}}}
\def\bmA{{\skew0\bm{A}}}
\def\bmB{{\skew0\bm{B}}}
\def\bmD{{\skew0\bm{D}}}
\def\bmI{{\skew0\bm{I}}}
\def\bmV{{\skew0\bm{V}}}
\def\bmU{{\skew0\bm{U}}}
\def\bv{{\skew0\bm{v}}}
\def\bw{{\skew0\bm{w}}}
\def\bmm{{\skew0\bm{m}}}
\def\bmzero{{\skew0\bm{0}}}
\def\bx{{\skew0\bm{x}}}
\def\xveca{{\skew0\bm{x}}_1}
\def\xvecb{{\skew0\bm{x}}_2}

\def\N{{\skew0\mathcal{N}}}
\def\T{{\small T}}

\def\mvec{{\skew0\bm{m}}}
\def\bmmu{{\skew0\bm{\mu}}}
\def\muvec{{\skew0\bm{\mu}}}
\def\balpha{{\skew0\bm{\alpha}}}
\def\bbeta{{\skew0\bm{\beta}}}
\def\bmtheta{{\skew0\bm{\theta}}}
\def\btheta{{\skew0\bm{\theta}}}

\def\cvec{{\skew0\mathbf{c}}}

\def\Xbar{\overline{X}}

\definecolor{lightgray}{rgb}{0.91,0.91,0.91}
\definecolor{purpleblue}{rgb}{0.50,0.50,1.00}



\usepackage{fontspec}
%\setsansfont{Fira Sans}
%\setmonofont{Fira Mono}
\setsansfont[ItalicFont={Fira Sans Light Italic},BoldFont={Fira Sans},BoldItalicFont={Fira Sans Italic}]{Fira Sans Light}
\setmonofont[BoldFont={Fira Mono Medium}]{Fira Mono}


\setbeamercolor{itemize item}{fg=myblue}
\setbeamertemplate{itemize item}[square]

\setbeamertemplate{navigation symbols}{\usebeamercolor[fg]{title in head/foot}\usebeamerfont{title in head/foot}\insertframenumber}
\setbeamertemplate{footline}{}

\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{exercise}[theorem]{Exercise}

\titlegraphic{\hfill\includegraphics[height=1cm]{mcgill_logo.png}}


%% You also use hyperref, and pick colors 
\hypersetup{colorlinks,citecolor=orange,filecolor=red,linkcolor=brown,urlcolor=blue}

\newcommand {\framedgraphiccaption}[2] {
	\begin{figure}
		\centering
		\includegraphics[width=\textwidth,height=0.8\textheight,keepaspectratio]{#1}
		\caption{#2}
	\end{figure}
}

\newcommand {\framedgraphic}[1] {
	\begin{figure}
		\centering
		\includegraphics[width=\textwidth,height=0.9\textheight,keepaspectratio]{#1}
	\end{figure}
}


\AtBeginSection[]{
	\begin{frame}
		\vfill
		\centering
		\begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
			\usebeamerfont{title}\insertsectionhead\par%
		\end{beamercolorbox}
		\vfill
	\end{frame}
}

\newcommand\Wider[2][3em]{%
	\makebox[\linewidth][c]{%
		\begin{minipage}{\dimexpr\textwidth+#1\relax}
			\raggedright#2
		\end{minipage}%
	}%
}



\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\red}[1]{\textcolor{red}{#1}}
%\makeatother

\usepackage{xparse}
\NewDocumentCommand\mylist{>{\SplitList{;}}m}
{
	\begin{itemize}
		\ProcessList{#1}{ \insertitem }
	\end{itemize}
}
\NewDocumentCommand\mynum{>{\SplitList{;}}m}
{
	\begin{enumerate}
		\ProcessList{#1}{ \insertitem }
	\end{enumerate}
}
\newcommand\insertitem[1]{\item #1}

\newcommand\FrameText[1]{%
	\begin{textblock*}{\paperwidth}(0pt,\textheight)
		\raggedright #1\hspace{.5em}
\end{textblock*}}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}
%\sffamily



%\title{Introduction to Regression Trees}
%\author{Sahir Bhatnagar \inst{1}}
%\author[shortname]{Sahir Rai Bhatnagar, PhD Candidate (Biostatistics) }
%\institute[shortinst]{Department of Epidemiology, Biostatistics and Occupational Health}

\title{Midterm Review}
\subtitle{}
\author{Sahir Bhatnagar and James Hanley}
\institute{
	EPIB 607\\
	Department of Epidemiology, Biostatistics, and Occupational Health\\
	McGill University\\
	
	\vspace{0.1 in}
	
	\texttt{sahir.bhatnagar@mcgill.ca}\\
	\texttt{\url{https://sahirbhatnagar.com/EPIB607/}}}

%\date

\maketitle


\begin{frame}{Exam Details}

\begin{itemize}
		\setlength\itemsep{.51em}
			\item \textbf{When:} Monday October 29, 11:30 am. McMED 504 (McIntyre Medical Building)
	\item This is a 2 hour, open book exam. 
	\item Calculators are permitted. Cellular phones \underline{are not} permitted. 
	\item The exam is out of 100. Write down all your answers in the provided booklet. 
	\item Provide units and state your assumptions when applicable. 
	\item If a question requires use of the $z$ or $t$ probabilites/quantiles, write the corresponding R code instead. Some commonly used quantiles are provided. 

\end{itemize}

\end{frame}


\section{Topics to be covered}

\begin{frame}{Topics to be covered}

\begin{enumerate}
	\setlength\itemsep{.51em}
	\item Data visualization (1)
	\item Descriptive statistics (4)
	\item Sampling Distributions, CLT, Confidence intervals and p-values 
	\item One sample mean (5)
	\item One sample proportion (6)
	\item Power (1)
	\item Bootstrap (1)
\end{enumerate}

\end{frame}

\section{Data visualization}

\begin{frame}{Aesthetics}
\begin{itemize}
	\setlength\itemsep{.51em}
	\item Aesthetics
		\framedgraphic{scales.jpg}
		\pause 
	\item Commonly used aesthetics in data visualization: position, shape, size, color, line width, line type. Some of these aesthetics can represent both continuous and discrete data (position, size, line width, color) while others can only represent discrete data (shape, line type)	
\end{itemize}
\end{frame}



\begin{frame}{Variable Types}
\begin{itemize}
	\setlength\itemsep{1.5em}
	\item quantitative/numerical continuous (1.3, 5.7, 83, $1.5\times 10^{-2}$)
	\item quantitative/numerical discrete (1,2,3,4)
	\item qualitative/categorical unordered (dog, cat, fish)
	\item qualitative/categorical ordered (good, fair, poor)
\end{itemize}



\end{frame}




\begin{frame}[fragile]{Color Palettes: Cynthia Brewer}

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\begin{alltt}
\hlstd{pacman}\hlopt{::}\hlkwd{p_load}\hlstd{(RColorBrewer)}
\hlstd{RColorBrewer}\hlopt{::}\hlkwd{display.brewer.all}\hlstd{()}
\end{alltt}


{\centering \includegraphics[width=1\linewidth]{figure/unnamed-chunk-1-1} 

}



\end{knitrout}


\end{frame}



\begin{frame}[fragile]{Color Palettes: viridis}



\framedgraphic{viridis.png}


\end{frame}



\section{Descriptive statistics}


\begin{frame}{Descriptive statistics}
\begin{itemize}
	\setlength\itemsep{1.5em}
	\item Boxplots, histograms, density plot
	\item IQR, median, mode, mean, min, max
	\item Q1, Q3
	\item Skewness (long left/right tail)
\end{itemize}
\end{frame}



\begin{frame}{Standard error (SE) of a sample statistic}
\begin{itemize}
	\item Recall: When we are talking about the variability of a
	\textbf{statistic}, we use the term \textbf{standard error} (not
	standard deviation). The standard error of the sample mean is $\sigma/\sqrt{n}$.
\end{itemize}


\begin{remarkm}[SE vs. SD]
	\begin{center}
		In quantifying the instability of the sample mean ($\bar{y}$) statistic,
		we talk of SE of the mean (SEM) \\ \ \\
		SE($\bar{y}$) describes how far $\bar{y}$ could (typically) deviate from $\mu$; \\ \ \\
		SD($y$) describes how far an individual $y$ (typically) deviates from $\mu$ (or from $\bar{y}$).
	\end{center}
\end{remarkm}	


\end{frame}



\section{Sampling Distributions, CLT, Confidence Intervals and p-values}


\begin{frame}{Parameters,  Samples,  and  Statistics}
\begin{itemize}
	\item \textbf{Paramter}: An  unknown  numerical  constant  pertaining  to  a  population/universe,  or  in  a  statistical  model. 
	\begin{itemize}
		\item $\mu$: population mean $\qquad\qquad$ $\pi$: population proportion
	\end{itemize}
	\pause
	\item \textbf{Statistic}: A  numerical  quantity  calculated  from  a  sample. The  empirical counterpart of the parameter,  used  to  \textit{estimate}  it.
	\pause
	\begin{itemize}
		\item $\bar{y}$: sample mean $\qquad\qquad$ $p$: sample proportion
	\end{itemize}
\end{itemize}

\pause
\Wider[4em]{
	\centering
	\includegraphics[scale=0.35]{../sampling_dist/MeansFig1.png}
}


\end{frame}


\frame{\frametitle{Samples must be random} 
	
	\begin{itemize}
		\item 	The validity of inference will depend on the
		way that the sample was collected. If a sample was collected badly, no amount of
		statistical sophistication can rescue the study. \\ \ \\ 
		
		\item Samples should be \textbf{random}. That is, there should be no systematic set of
		characteristics that is related to the scientific question of interest that causes some
		people to be more likely to be sampled than others. The simplest type of randomization
		selects members from the population with equal probability (a uniform distribution). \\ \
		\\ 
		
		\pause
		
		\item 	\textbf{Do not cheat by}  
		\begin{itemize}
			\item 	Taking 5 people from the \emph{same} household to estimate
			
			\begin{itemize}
				\item proportion of Québécois who don't have a family doctor
				\item who saw a medical doctor last year
				\item average rent
			\end{itemize}  
			
		
		
			
			\item Sampling the depth of the ocean \emph{only around Montreal} to estimate \begin{itemize}
				\item proportion of  Earth's  surface  covered  by  water
			\end{itemize} 
			
		\end{itemize}
	\end{itemize}
}






\frame{\frametitle{Sampling Distributions} 
	
\begin{defm}[Sampling Distribution]
	\begin{itemize}
		\setlength\itemsep{1.5em}
		\item The sampling distribution of a statistic is the distribution of values taken by the statistic in \textbf{all possible samples of the same size} from the same population.
		\item The standard deviation of a sampling distribution is called a \textbf{standard error}
	\end{itemize} 
\end{defm}
			
	
	
}


\begin{frame}[fragile]{Sampling Distributions}

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figure/unnamed-chunk-2-1} 

}

\caption[Ideal world]{Ideal world. Sampling distributions are obtained by drawing repeated samples from the population, computing the statistic of interest for each, and collecting (an infinite number of) those statistics as the sampling distribution}\label{fig:unnamed-chunk-2}
\end{figure}


\end{knitrout}

\end{frame}


\frame{\frametitle{Why are sampling distributions important?} 
	
	\begin{itemize}
		\setlength\itemsep{2em} 
		\item They  tell  us  how  far  from  the  target  (true  value  of  the  parameter)  our  statistical  \emph{shot}  at  it  (i.e.  the  statistic  calculated  form  a  sample)  is  likely  to  be,  or, to  have  been.  \pause 
		
		\item Thus,  they  are  used  in  confidence  intervals  for  parameters. Specific  sampling  distributions  (based  on  a null value  for  the  parameter)  are also  used  in  statistical  tests  of  hypotheses.
		
	\end{itemize}	
	
	
}


\begin{frame}[fragile]{Sampling distribution: mean depth of the ocean}



\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=1\linewidth]{figure/unnamed-chunk-4-1} 

}



\end{knitrout}

\end{frame}



\begin{frame}[fragile]{Sampling distribution: proportion covered by water}




\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=1\linewidth]{figure/unnamed-chunk-6-1} 

}



\end{knitrout}

\end{frame}




\begin{frame}[fragile]{Normal Distribution: For probabilities we use $pnorm$}


\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\begin{alltt}
\hlstd{stats}\hlopt{::}\hlkwd{pnorm}\hlstd{(}\hlkwc{q} \hlstd{=} \hlnum{130}\hlstd{,} \hlkwc{mean} \hlstd{=} \hlnum{100}\hlstd{,} \hlkwc{sd} \hlstd{=} \hlnum{13}\hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] 0.9894919
\end{verbatim}

\end{knitrout}


\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\begin{alltt}
\hlstd{mosaic}\hlopt{::}\hlkwd{xpnorm}\hlstd{(}\hlkwc{q} \hlstd{=} \hlnum{130}\hlstd{,} \hlkwc{mean} \hlstd{=} \hlnum{100}\hlstd{,} \hlkwc{sd} \hlstd{=} \hlnum{13}\hlstd{)}
\end{alltt}


{\centering \includegraphics[width=0.6\linewidth]{figure/probs3-1} 

}


\begin{verbatim}
## [1] 0.9894919
\end{verbatim}

\end{knitrout}


\begin{itemize}
	\item \texttt{pnorm} returns the integral from $-\infty$ to $q$ for a $\mathcal{N}(\mu, \sigma)$
	\item \texttt{pnorm} goes from \textit{quantiles} (think $Z$ scores) to probabilities
\end{itemize}

\end{frame}



\begin{frame}[fragile]{Normal Distribution: For quantiles we use $qnorm$}



\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\begin{alltt}
\hlstd{stats}\hlopt{::}\hlkwd{qnorm}\hlstd{(}\hlkwc{p} \hlstd{=} \hlnum{0.0104}\hlstd{,} \hlkwc{mean} \hlstd{=} \hlnum{100}\hlstd{,} \hlkwc{sd} \hlstd{=} \hlnum{13}\hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] 69.94926
\end{verbatim}

\end{knitrout}

 

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\begin{alltt}
\hlstd{mosaic}\hlopt{::}\hlkwd{xqnorm}\hlstd{(}\hlkwc{p} \hlstd{=} \hlnum{0.0104}\hlstd{,} \hlkwc{mean} \hlstd{=} \hlnum{100}\hlstd{,} \hlkwc{sd} \hlstd{=} \hlnum{13}\hlstd{)}
\end{alltt}


{\centering \includegraphics[width=0.6\linewidth]{figure/probs5-1} 

}


\begin{verbatim}
## [1] 69.94926
\end{verbatim}

\end{knitrout}



\small{
\begin{itemize}
	\item \texttt{qnorm} answers the question: What is the Z-score of the $p$th percentile of the normal distribution?
	
	\item \texttt{qnorm} goes from \textit{probabilities} to quantiles 
\end{itemize}
}
\end{frame}

\begin{frame}[fragile]{Empirical Rule or 68-95-99.7\% Rule}

\framedgraphic{../sampling_dist/6899rule.png}

\end{frame}



\begin{frame}[fragile]{Quadruple the work, half the benefit}

\framedgraphiccaption{../sampling_dist/ROOToceanAll.png}{When the sample size increases from 4 to 16, the spread of the sampling distribution for the mean is reduced by a half, i.e., the range is cut in half. This is known as the curse of the $\sqrt{n}$}
\end{frame}


\frame{\frametitle{The Central Limit Theorem (CLT)} 
	
	\begin{itemize}
		\item The sampling distribution of $\bar{y}$ is, for a large enough $n$, close to Gaussian in shape no matter what the shape of the distribution of individual $Y$ values. 
		\item This phenomenon is referred to as the CENTRAL LIMIT THEOREM 
		\item The CLT applied also to a \underline{sample proportion}, \underline{slope}, \underline{correlation}, or any other statistic created by \underline{aggregation of individual observations}
	\end{itemize}
	
	\begin{thm}[Central Limit Theorem]
		\begin{center}
			if $Y \sim ???(\mu_Y, \sigma_Y)$, then \\ \ \\
			$\bar{y} \sim \mathcal{N}(\mu_Y, \sigma_Y / \sqrt{n})$
		\end{center}
	\end{thm}
	
	\vspace{1.25cm}
	%pause
}

\begin{frame}{Confidence Interval}

\begin{defm}[Confidence Interval]
	A level $C$ confidence interval for a parameter has two parts:
	\begin{enumerate}
		\item An interval calculated from the data, \underline{usually} of the form $$\textrm{estimate} \pm \textrm{margin of error}$$ where the estimate is a sample statistic and the margin of error represents the accuracy of our guess for the parameter.
		\item A confidence level $C$, which gives the probability that the interval will capture the true parameter value in \textit{different possible samples}. That is, the confidence level is the success rate for the method
	\end{enumerate}
\end{defm}

%\framedgraphic{6899rule.png}

\end{frame}

\frame{\frametitle{Confidence Interval: A simulation study}

\vspace*{-0.1in}

\begin{figure}
	\begin{center}
		\epsfig{figure=../sampling_dist/Part3Figs/CIplots.eps,width=3.2in,height=2.7in}
		\caption{\small{True parameter value is 2 (red line). Each horizontal black line represents a 95\% CI from a sample and contains the true parameter value. The blue CIs do not contain the true parameter value. 95\% of all samples give an interval that contains the population parameter.}}
	\end{center}
\end{figure}
}


\begin{frame}{Interpreting a frequentist confidence interval}
\begin{itemize}
	\setlength\itemsep{1em}
	\item The confidence level is the success rate of the method that produces the interval. \pause
	\item We don't know whether the 95\% confidence interval from a \underline{particular
		sample} is one of the 95\% that capture $\theta$ (the unknown population parameter), or one of the unlucky 5\% that miss. \pause
	\item To say that we are \underline{95\% confident} that the unknown value of $\theta$
	lies between $U$ and $L$ is shorthand for ``We got these numbers using a
	method that gives correct results 95\% of the time.''
\end{itemize}
\end{frame}

\begin{frame}[fragile]{68\% Confidence interval using \texttt{qnorm}}




\vspace*{-0.09in}

\Wider[2em]{
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figure/unnamed-chunk-8-1} 

}

\caption{68\% Confidence interval calculated using  \mbox{\texttt{qnorm(p = c(0.16,0.84), mean = 37, sd = 4.2)}}}\label{fig:unnamed-chunk-8}
\end{figure}


\end{knitrout}
	
}
\end{frame}



\begin{frame}[fragile]{95\% Confidence interval using \texttt{qnorm}}

\vspace*{-0.09in}


\Wider[2em]{
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figure/unnamed-chunk-9-1} 

}

\caption{95\% Confidence interval calculated using  \mbox{\texttt{qnorm(p = c(0.025,0.975), mean = 37, sd = 4.2)}}}\label{fig:unnamed-chunk-9}
\end{figure}


\end{knitrout}

}

\end{frame}

\frame{\frametitle{Example: Inference for a single population mean} So what
	does the CI allow us to learn about $\mu$??
	\begin{itemize}
		\setlength\itemsep{2em}
		\item It tells us that if we repeated this procedure again and again
		(collecting a sample mean, and constructing a 95\% CI), 95\% of the
		time, the CI would \textit{cover} $\mu$. \pause 
		\item That is, with 95\% probability, the \textit{procedure}
		will include the true value of $\mu$. Note that we are making \underline{a probability statement about the CI}, not about the parameter. \pause
		\item Unfortunately, \textcolor{blue}{we do not know whether the true value of $\mu$ is
			contained in the CI in the particular experiment that we have
			performed.}
	\end{itemize}
}



\section{Bootstrap}

\begin{frame}{Motivation for the Bootstrap}
\begin{itemize}
	\setlength\itemsep{2em}
	\item The $\pm$ and \texttt{qnorm} methods to calculate a CI both require the CLT
\end{itemize}

\pause

\vspace*{0.2in}

\Large \textcolor{myblue}{Q: What happens if the CLT hasn't `kicked in`? Or you don't believe the CLT?} \\ \ \\
\pause 
\Large \textcolor{red}{A: Bootstrap} \\ \ \\
\end{frame}


\section{The Bootstrap}


\begin{frame}[fragile]{Ideal world: known sampling distribution}

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figure/unnamed-chunk-10-1} 

}

\caption{\scriptsize{Ideal world. Sampling distributions are obtained by drawing repeated samples from the population, computing the statistic of interest for each, and collecting (an infinite number of) those statistics as the sampling distribution}}\label{fig:unnamed-chunk-10}
\end{figure}


\end{knitrout}

\end{frame}



\begin{frame}[fragile]{Reality: use the bootstrap distribution instead}




\framedgraphiccaption{../bootstrap/boot_diag.pdf}{\scriptsize{Bootstrap world. The bootstrap distribution is obtained by drawing repeated samples from an estimate of the population, computing the statistic of interest for each, and collecting those statistics. The distribution is centered at the observed statistic ($\bar{y}$), not the parameter ($\mu$).}}

\end{frame}


\begin{frame}[fragile]{Main idea: simulate your own sampling distribution}



\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\begin{alltt}
\hlkwd{library}\hlstd{(mosaic)}
\hlstd{s_dist} \hlkwb{<-} \hlkwd{do}\hlstd{(}\hlnum{10000}\hlstd{)} \hlopt{*} \hlkwd{mean}\hlstd{(} \hlopt{~} \hlstd{alt,} \hlkwc{data} \hlstd{=} \hlkwd{resample}\hlstd{(depths.n.20))}
\hlstd{CI_95} \hlkwb{<-} \hlkwd{quantile}\hlstd{(}\hlopt{~} \hlstd{mean,} \hlkwc{data} \hlstd{= s_dist,} \hlkwc{probs} \hlstd{=} \hlkwd{c}\hlstd{(}\hlnum{0.025}\hlstd{,} \hlnum{0.975}\hlstd{))}
\end{alltt}


{\centering \includegraphics[width=1\linewidth]{figure/unnamed-chunk-13-1} 

}



\end{knitrout}

\end{frame}


\section{One sample mean}


\begin{frame}{$\sigma$ known vs. unknown}
\begin{center}
	\begin{tabular}{|l|c|c|} \hline
		$\sigma$& known & unknown \\ \hline Data & $\{y_1,y_2,...,y_n\}$ &
		$\{y_1,y_2,...,y_n\}$\\
		& & \\
		Pop'n param & $\mu$ & $\mu$\\
		& & \\
		Estimator & $\overline{y} = \frac{1}{n}\sum_{i=1}^n y_i$ & $\overline{y} = \frac{1}{n}\sum_{i=1}^n y_i$ \\
		& & \\
		SD & $\sigma$ & $s = \sqrt{\frac{\sum_{i=1}^n(y_i-\overline{y})^2}{n-1}}$ \\
		& & \\
		SEM & $\sigma/\sqrt{n}$ & $s / \sqrt{n}$ \\
		& & \\
		$(1-\alpha)100$\% CI & $\overline{y} \pm z^\star_{1-\alpha/2}$(SEM) & $\overline{y} \pm t^\star_{1-\alpha/2, (n-1)}$(SEM) \\
		& & \\
		test statistic & $\frac{\overline{y}-\mu_0}{\textrm{SEM}}\sim \mathcal{N}(0,1)$ &
		$\frac{\overline{y}-\mu_0}{\textrm{SEM}}\sim t_{(n-1)}$ \\
		\hline
	\end{tabular}
\end{center}
\end{frame}


\begin{frame}{Assumptions}
\Wider[3em]{
\begin{center}
	\begin{tabular}{|l|c|c|c|} \hline
		& $z$ & $t$ & Bootstrap \\ 
		\hline 
		SRS & \cmark &\cmark &	\cmark\\
		& & & \\
		Normal population & \cmark$^\star$ & \cmark$^\star$ &  \xmark\\
		& & &\\
		needs CLT &  \cmark$^\star$ & \cmark$^\star$ &  \xmark\\
		& & &\\
		$\sigma$ known  & \cmark & \xmark & \xmark\\
		& & &\\
		Sampling dist. center at & $\mu$ & $\mu$ & $\bar{y}$\\
		& & &\\
		SD & $\sigma$ & $s$ & $s$ \\
		& & &\\
		SEM & $\sigma/\sqrt{n}$ & $s / \sqrt{n}$ & SD(bootstrap statistics) \\
		\hline
	\end{tabular}

\footnotetext[1]{*If population is Normal then CLT is not needed. If population is not Normal then CLT is needed.}
\end{center}
}
\end{frame}


\section{p-values}

\begin{frame}
\frametitle{$p$-values and statistical tests}


%\vspace{18pt}
\begin{defm}[$p$-value]
	A \textbf{probability concerning the observed data}, calculated under a \textbf{Null Hypothesis} assumption, i.e., assuming that the only factor operating is sampling or measurement variation. 
\end{defm}

\begin{itemize} 
	\item[\underline{Use}] To assess the evidence provided by the sample data
	in relation to a pre-specified claim or `hypothesis' concerning some parameter(s) or data-generating process. 
	\item[\underline{Basis}] As with a confidence interval, it makes use of the concept of a \textit{distribution}. 
	\item[\underline{Caution}] A $p$-value is NOT the probability that the null `hypothesis' is true
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{More about the $p$-value}
\small
%\begin{footnotesize}
\begin{itemize}
	\setlength\itemsep{.3em}
	\item The $p$-value is a \textbf{probability concerning data}, \textbf{conditional on the Null Hypothesis being true}. \pause
	\item \textbf{It is not the probability that Null Hypothesis is true}, \textit{conditional on the data.} \pause
	%\item Very few MDs mix up complement of specificity (i.e. probability of a `positive'  test result when in fact patient does not have disease in question) with positive predictive value (i.e. probability that a patient who has had a `positive'  test result does  have disease in question).
	\begin{align*}
	p_{value} & = P(\textrm{this or more extreme data}| H_0) \\
	& \neq P(H_0|\textrm{this or more extreme data}).
	\end{align*}
	
	\pause 
	\item Statistical tests are often coded as statistically significant or not according to whether results are extreme or not with respect to a reference (null)  distribution.  But a test result  is just one piece of data, and needs to be considered \textit{along with  rest of evidence} before coming to a `conclusion.' \item \textbf{Likewise with statistical `tests': the $p$-value is just one more piece of \textit{evidence}, hardly enough to `conclude' anything}. 
	%\item The probability that the DNA from the blood  of a randomly selected (innocent) person would match that from blood on crime-scene glove was 
	%$p=10^{-17}$. \textit{Do not equate this} Prob[data $|$ innocent] \textit{with its transpose}:
	%writing ``data'' as shorthand for ``this or more extreme data'', we need to be aware that 
	%$$p_{value} = Prob[ \ data \  | \  H_0 ] \neq Prob[ \ H_0 \  |  \ data ].$$
\end{itemize}
%\end{footnotesize}
\end{frame}

\begin{frame}
\frametitle{Close relationship between $p$-value and CI}
\begin{center}
	\includegraphics[width=1.65in]{../sample_size/P-CI.pdf}
\end{center} 
\begin{footnotesize}
	\begin{itemize}
		\item
		(Upper graph) If upper limit of 95\% CI\textit{ just touches} null value, then
		the 2 sided $p$-value is 0.05 (or 1 sided $p$-value is 0.025). 
		\item
		(Lower graph) If upper limit \textit{excludes} null value, then
		the 2 sided $p$-value is less than 0.05 (or 1 sided $p$-value is less than 0.025). 
		\item
		(Graph not shown) If  CI \textit{includes} null value, then the 2-sided $p$-value is greater than (the conventional) 0.05, and thus observed statistic is ``not statistically significantly different'' from hypothesized null value. 
	\end{itemize}
\end{footnotesize}
\end{frame}


\section{Power and sample size}

\begin{frame}[fragile]{Power = $1 - \beta$}

\vspace*{-0.2in}

\begin{defm}[Power = $1-\beta$]
	The probability that a fixed level $\alpha$ significance test will reject $H_0$ when a particular alternative value of the parameter is true is called the \textbf{power} of the test to detect the alternative. 
\end{defm}


\vspace*{-0.08in}

\centering
\includegraphics[scale=0.31]{../sample_size/HypTest3-3.pdf}


\end{frame}

\begin{frame}{Power and Sample Size: 3 questions}

\begin{enumerate}
	\setlength\itemsep{1em}
	\item How much water a supplier could add to the milk before they have a 10\% , 50\%, 80\%
	chance of getting caught, i.e., of the buyer detecting the cheating ? \pause
	\item Assume a 99:1 mix of milk and water. What are the chances of detecting cheating if the buyer uses samples $n$=10, 15 or 20 rather than just 5 measurements? \pause
	\item At what $n$ does the chance of detecting cheating reach 80\%? (\textit{a commonly used, but arbitrary, criterion used in sample-size planning by investigators seeking funding for their proposed research})
\end{enumerate}

\end{frame}


\begin{frame}[fragile]{If the supplier added 1\% water to the milk}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=1\linewidth]{figure/unnamed-chunk-14-1} 

}



\end{knitrout}
\end{frame}



\begin{frame}

\begin{center}
	\includegraphics[scale=0.45]{../sample_size/ProbDetectingWaterInMilk.pdf} 
\end{center}

\vspace*{-0.18in}

{ \footnotesize
	The probabilities in red were calculated using the formula:
	\texttt{stats::pnorm(cutoff, mean = mu.mixture, sd = SEM, lower.tail=FALSE)}
}
\end{frame}


\begin{frame}
\begin{center}
	\includegraphics[scale=0.5]{../sample_size/SampleSize1pctWaterAdded.pdf} 
\end{center}
\end{frame}

\begin{frame}[fragile]{The balancing formula}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=1\linewidth]{figure/unnamed-chunk-15-1} 

}



\end{knitrout}
\end{frame}


\begin{frame}{What sample size needed?}

\begin{itemize}
\setlength\itemsep{1em}
\item The `balancing formula', in SEM terms, is simply the $n$ where
$$ 1.645 \times SEM + 0.84 \times SEM = \Delta.$$
Replacing each of the  SEMs (assumed equal, because we assumed the variability
is approx. the same under both scenarios) by $\sigma/\sqrt{n}$,  i.e.,

$$ 1.645 \times \sigma/\sqrt{n} + 0.84 \times \sigma/\sqrt{n} = \Delta.$$

and solving for $n$, one gets

$$  n = (1.645 + 0.84)^2  \times \bigg\{ \frac{\sigma}{\Delta} \bigg\}^2 = 
(1.645 + 0.84)^2  \times \bigg\{ \frac{Noise}{Signal} \bigg\}^2 .$$
\end{itemize}

\end{frame}

\begin{frame}{What sample size needed? General Formula}

\begin{itemize}
	\setlength\itemsep{2em}
	\item Two sided alternative:
	$$\Delta = z_{1-\alpha/2} \times SEM + z_{1-\beta} \times SEM$$
	
		\item One sided alternative:
	$$\Delta = z_{1-\alpha} \times SEM + z_{1-\beta} \times SEM$$
\end{itemize}

\end{frame}


\section{One sample proportion}


\begin{frame}
\Wider[5em]{	
	\includegraphics[width=4.95in,height=3.7in]{../one_sample_prop/Nomogram.pdf}
}
\end{frame}



\section{Examples}



\begin{frame}{Comparing two sun block lotions}

\begin{example}
	Your company produces a sun block lotion designed to protect the skin from both UVA and UVB exposure to the sun. You hire a company to compare your product with the product sold by your major competitor. The testing company exposes skin on the back of a sample of 20 people to UVA and UVB rays and measures the protection provided by each product. For 13 of the subjects, your product provided better protection. Do you have evidence to support a commercial claiming that your product provides superior UVA and UVB protection?
\end{example}


\end{frame}



\begin{frame}[fragile]{Comparing two sun block lotions}
\small
\begin{enumerate}
	\setlength\itemsep{1em}
	\item State the null hypothesis in words. \pause
	\item State the hypotheses in statistical notation \pause
	\begin{itemize}
			\setlength\itemsep{.71em}
		\item We need to first define the reference (null) distribution. Then the parameter of interest \pause
		\item Binomial(n=20, $\pi$=0.5) is the reference distribution where $\pi$ is the proportion of people who would receive superior UVA and UVB protection from your product. \pause
		The following are all equivalent: 
		\item[] $H_0: \pi = 0.5 \qquad$ $H_a: \pi > 0.5$ \pause
		\item[] $H_0: \pi_{\textrm{your product}}=\pi_{\textrm{their product}} = 0.5 \qquad$ $H_a: \pi_{\textrm{your product}} > \pi_{\textrm{their product}}$ \pause
		\item[] $H_0: \pi_{\textrm{your product}}-\pi_{\textrm{their product}} = 0 \qquad$ $H_a: \pi_{\textrm{your product}} > \pi_{\textrm{their product}}$ \pause
		\item You must define your own $\alpha$. Here we choose $\alpha=0.05$ 		
	\end{itemize}
      
 
\end{enumerate}

\end{frame}




\begin{frame}[fragile]{Comparing two sun block lotion - p-value}
\small
\begin{enumerate}
	\setlength\itemsep{.51em}
	
	\item Exact $p$-value:
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\begin{alltt}
\hlkwd{pbinom}\hlstd{(}\hlnum{12}\hlstd{,} \hlnum{20}\hlstd{,} \hlnum{0.5}\hlstd{,} \hlkwc{lower.tail} \hlstd{=} \hlnum{FALSE}\hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] 0.131588
\end{verbatim}
\begin{alltt}
\hlnum{1} \hlopt{-} \hlkwd{pbinom}\hlstd{(}\hlnum{12}\hlstd{,} \hlnum{20}\hlstd{,} \hlnum{0.5}\hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] 0.131588
\end{verbatim}

\end{knitrout}
	\pause 

	\item Approximate $p$-value assuming Normal approximation is ok ($20 \times 0.5 \geq 10$ and $20 \times (1-0.5) \geq 10$)
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\begin{alltt}
\hlstd{SEp} \hlkwb{<-} \hlkwd{sqrt}\hlstd{(}\hlnum{0.5}\hlopt{*}\hlnum{0.5}\hlopt{/}\hlnum{20}\hlstd{)} \hlcom{# under the null}
\hlstd{zstat} \hlkwb{<-} \hlstd{(}\hlnum{0.65} \hlopt{-} \hlnum{0.5}\hlstd{)} \hlopt{/} \hlstd{SEp}
\hlkwd{pnorm}\hlstd{(zstat,} \hlkwc{lower.tail} \hlstd{=} \hlnum{FALSE}\hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] 0.08985625
\end{verbatim}

\end{knitrout}
	
	
	
\end{enumerate}

\end{frame}




\begin{frame}[fragile]{Comparing two sun block lotion - Exact 95\% CI}
\small
\begin{enumerate}
	\setlength\itemsep{1em}
	
	\item Exact CI (Clopper-Pearson or Nomogram):
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\begin{alltt}
\hlstd{mosaic}\hlopt{::}\hlkwd{binom.test}\hlstd{(}\hlkwc{x} \hlstd{=} \hlnum{13}\hlstd{,} \hlkwc{n} \hlstd{=} \hlnum{20}\hlstd{,} \hlkwc{p} \hlstd{=} \hlnum{0.5}\hlstd{,}
\hlkwc{ci.method} \hlstd{=} \hlstr{"Clopper-Pearson"}\hlstd{,}
\hlkwc{alternative} \hlstd{=} \hlstr{"greater"}\hlstd{)}
\end{alltt}
\begin{verbatim}



data:  13 out of 20
number of successes = 13, number of trials = 20, p-value = 0.1316
alternative hypothesis: true probability of success is greater than 0.5
95 percent confidence interval:
 0.4419655 1.0000000
sample estimates:
probability of success 
                  0.65 
\end{verbatim}

\end{knitrout}
	

	
\end{enumerate}

\end{frame}







\begin{frame}[fragile]{Comparing two sun block lotion - Approximate 95\% CI}
\small
\begin{enumerate}
	\setlength\itemsep{1em}
	
	\item Approximate 95\% CI:
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\begin{alltt}
\hlstd{mosaic}\hlopt{::}\hlkwd{binom.test}\hlstd{(}\hlkwc{x} \hlstd{=} \hlnum{13}\hlstd{,} \hlkwc{n} \hlstd{=} \hlnum{20}\hlstd{,} \hlkwc{p} \hlstd{=} \hlnum{0.5}\hlstd{,}
\hlkwc{ci.method} \hlstd{=} \hlstr{"Wald"}\hlstd{,}
\hlkwc{alternative} \hlstd{=} \hlstr{"greater"}\hlstd{)}
\end{alltt}
\begin{verbatim}

	Exact binomial test (with Wald CI)

data:  13 out of 20
number of successes = 13, number of trials = 20, p-value = 0.1316
alternative hypothesis: true probability of success is greater than 0.5
95 percent confidence interval:
 0.4745704 1.0000000
sample estimates:
probability of success 
                  0.65 
\end{verbatim}

\end{knitrout}
	
		
	\item Approximate 95\% CI assuming Normal approximation is ok 
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\begin{alltt}
\hlkwd{qnorm}\hlstd{(}\hlkwd{c}\hlstd{(}\hlnum{0.025}\hlstd{,} \hlnum{0.975}\hlstd{),} \hlkwc{mean} \hlstd{=} \hlnum{0.65}\hlstd{,} \hlkwc{sd} \hlstd{=} \hlkwd{sqrt}\hlstd{(}\hlnum{0.65}\hlopt{*}\hlnum{0.35} \hlopt{/} \hlnum{20}\hlstd{))}
\end{alltt}
\begin{verbatim}
## [1] 0.4409627 0.8590373
\end{verbatim}

\end{knitrout}
	
\end{enumerate}

\end{frame}




\end{document}
