---
author: Sahir Bhatnagar
title: "Week 11: Convergence in Distribution and Central Limit Theorem"
subtitle: MATH697
date: "November 14, 2017"
output:
  beamer_presentation:
    keep_tex: yes
    theme: metropolis
    latex_engine: xelatex
    slide_level: 2
    incremental: no
    includes:
      in_header: /home/sahir/Dropbox/PhD/Year4/qls/slides/week3/header.tex
fontsize: 12pt
classoption: compress
editor_options: 
  chunk_output_type: console
---

```{r,setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(cache=TRUE, message = FALSE, tidy = FALSE)
pacman::p_load(cowplot)
pacman::p_load(sjPlot)
```

# Convergence in Distribution

## Convergence in Distribution

\begin{definition}[Convergence in Distribution]
Let $X_1, X_2, \ldots$ be a sequence of random variables. Then we say that the \textbf{sequence converges in distribution} to a random variable $X$ if 
\[\lim_{n \to \infty} F_{X_n}(x) = F_X(x)\]
at all points $x$ where $F_X(x)$ is continuous and we write $X_n \overset{D}{\to} X$
\end{definition}

\pause 

- Intuitively, $X_1, X_2, \ldots$ converges in distribution to $X$ if for large $n$, the distribution of $X_n$
is close to that of $X$  
- The importance of this, is that often the distribution of $X_n$ is difficult to work with, while that of $X$ is much simpler  
- With $X_n$ converging in distribution to $X$, however, we can approximate the distribution of $X_n$ by that of $X$.  


## Convergence in Probability Implies in Distribution

\begin{theorem}[Convergence in Probability Implies in Convergence in Distribution]
If the sequence of random variables $X_1, X_2, \ldots$ converges to a random variable $X$, the sequence also converges in distribution to $X$. That is if $X_n \overset{P}{\to} X$ then $X_n \overset{D}{\to} X$
\end{theorem}


## Poisson Approximation to the Binomial

\begin{example}[Poisson Approximation to the Binomial]
Suppose that $X_n \sim Binomial(n, \lambda/n)$ and $X\sim Poisson(\lambda)$. We have previously seen that as $n \to \infty$
\[P(X_n = j) = \binom{n}{j} \left( \frac{\lambda}{n} \right)^j \left(1- \frac{\lambda}{n} \right)^{n-j} \to e^{-\lambda} \frac{\lambda^j}{j!} \]
This implies that $F_{X_n}(x) \to F_X(x)$ and thus $X_n \overset{D}{\to} X$
\end{example}


# Central Limit Theorem

## Introduction

- Let $X_1, X_2, \ldots,$ be iid with mean $\mu$ and finite variance $\sigma^2$. The law of large numbers says that as $n \to \infty$, $\Xbar_n$ converges to the constant $\mu$ (with probability 1).  
- But what is its distribution along the way to becoming a constant?  
- This is addressed by the central limit theorem (CLT)



## Central Limit Theorem (CLT)
\scriptsize
- The CLT states that for large $n$, the distribution of $\Xbar_n$ (after standardization) approaches a standard Normal distribution.  
- By standardization, we mean that we subtract $\mu$, the expected value of $\Xbar_n$, and divide by $\sigma/\sqrt{n}$, the standard deviation of $\Xbar_n$.  

\begin{theorem}[CLT]
As $n\to\infty$ \[\sqrt{n} \left( \frac{\Xbar_n - \mu}{\sigma}\right) \overset{D}{\to} \mathcal{N} (0,1) \] From this we can also say that for large $n$
\[\Xbar_n \sim \mathcal{N}(\mu, \sigma^2/n)\]
\end{theorem}

_Proof_:on board

\pause 

- In words, the CDF of the left-hand side approaches $\Phi$, the CDF of a standard Normal distribution.  
- Starting from virtually no assumptions (other than independence and finite variances), \textbf{we end up with normality}    


## Central Limit Theorem (CLT) for the Sum

- The CLT says that the sample mean $\Xbar_n$ is approximately Normal  
- But since the sum $$S_n = X_1 + \cdots + X_n = n \Xbar_n$$ is just a scaled version of $\Xbar_n$, the CLT also implies that $S_n$ is approximately Normal  
- If $X_j$ have mean $\mu$ and variance $\sigma^2$, $S_n$ has mean $n\mu$ and variance $n\sigma^2$. 
\begin{theorem}[CLT for the Sum]
for large $n$
\[S_n \sim \mathcal{N}(n\mu, n\sigma^2)\]
\end{theorem}



## Running Proportion of Heads Revisited

Before, we used the law of large numbers to conclude that $\Xbar_n \to 1/2$ as $n\to \infty$. Now, using the central limit theorem, we can say more: $E(\Xbar_n ) = 1/2$ and $Var(\Xbar_n) = 1/(4n)$, so for large n, \[\Xbar_n \sim \mathcal{N}\left(\frac{1}{2}, \frac{1}{4n}\right)\]


## Poisson Convergence to Normal

Let $Y\sim Pois(n)$. Using MGFs it can be shown that $Y$ can be expressed as a sum of $n$ iid Poisson(1) random variables. Therefore by CLT, for large $n$, \[Y \sim \mathcal{N}(n,n)\]


## Binomial Convergence to Normal

Let $Y\sim Binomial(n,p)$. Using MGFs it can be shown that $Y$ can be expressed as a sum of $n$ iid Bernoulli(p) random variables. Therefore by CLT, for large $n$, \[Y \sim \mathcal{N}(np,np(1-p))\]


## CLT Visual
Histograms of the distribution of $\Xbar_n$ for different starting distributions of the $X_j$ (indicated by the rows) and increasing values of $n$ (indicated by the columns). Each histogram is based on 10,000 simulated values of $\Xbar_n$. Regardless of the starting distribution of the $X_j$, the distribution of $\Xbar_n$ approaches a Normal distribution as $n$ grows.

\centering
```{r, out.width = "185pt", echo=FALSE, eval=TRUE}
knitr::include_graphics("clt.png")
```



## CLT Exercise
\small
One way to visualize the CLT for a distribution of interest is to plot the distribution of $\Xbar_n$ for various values of $n$. To do this, we first have to generate iid $X_1, \ldots, X_n$ a bunch of times from our distribution of interest. For example, suppose that our distribution of interest is $Unif(0, 1)$, and we are interested in the distribution of $\Xbar_{12}$ , i.e., we set $n = 12$. In the following code, we create a matrix of iid standard Uniforms. The matrix has 12 columns, corresponding to $X_1$ through $X_{12}$. Each row of the matrix is a different realization of $X_1$ through $X_{12}$.

```{r, eval=FALSE}
nsim <- 10^4
n <- 12
x <- matrix(runif(n*nsim), nrow=nsim, ncol=n)
xbar <- rowMeans(x)
hist(xbar)
# change runif for rexp. what do you notice?
```

## CLT In Action

\framedgraphic{clt2.png}

\footnotetext[1]{\scriptsize{http://www.medicine.mcgill.ca/epidemiology/Joseph/courses/EPIB-607/notes.pdf}}


## CLT In Action Exercise
\scriptsize

<!--In this exercise we will simulate the scenario illustrated on the previous slide. We will vary the size of $n$ to see what happens as $n$ goes large.  -->

1. Set $n = 10$    
2. Simulate data from various distributions, each representing a different part of the journey to McGill. Create a `data.frame` of the simulated times and add a column for the total transit time

```{r, eval=FALSE, echo=1:3}
walk <- rnorm(n, 4, 1) ; bus <- runif(n, 4, 16)
ride <- rpois(n, 8); climb <- rgamma(n, shape = 6, scale = 0.5)
fall <- rexp(n, rate = 4)
DT <- data.frame(walk, bus, ride, climb, fall)
DT$transit_time <- apply(DT,1,sum)
```

3. Calculate the theoretical means and variances for each of the distributions. Use the CLT to determine the mean and variance of the total transit time.  


```{r, eval=FALSE, echo=1:2}
mean.walk=4; mean.bus=(4+16)/2; mean.ride=8; mean.climb=6*0.5; mean.fall=1/4
var.walk=1; var.bus=(16-4)^2/12; var.ride=8; var.climb=6*0.5^2; var.fall=1/4^2
mean.transit <- mean.walk+mean.bus+mean.ride+mean.climb+mean.fall
var.transit <- var.walk+var.bus+var.ride+var.climb+var.fall
```

4. Plot a histogram of the total transit times and superimpose the density of the theoretical distribution of the sum. Repeat the whole exercise for $n=50, 100, 200, 500, 1000, 2000$.  

```{r, eval=FALSE}
hist(DT$transit_time, freq=FALSE)
curve(dnorm(x,mean=mean.transit,sd=sqrt(var.transit)),0,50,
      add=TRUE,lwd=2,col="red")
```


## Session Info

```{r}
devtools::session_info()
```








