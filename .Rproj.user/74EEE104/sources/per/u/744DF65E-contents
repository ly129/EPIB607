cases <- c(77, 61)
controls <- c(273, 289)
open <- c(1,0)
df <- as.data.frame(cbind(cases, controls, open))

fit <- glm(cbind(cases,controls) ~ open, data = df,
           family=binomial(link="logit"))
summary(fit)
predict(fit, type = "link")
predict(fit, type = "resp")

plogis(coef(fit)[1])
61/350
plogis(sum(coef(fit))) * 350
(1-plogis(coef(fit)[2])) * 350





par(mfrow=c(1, 2))
curve(log(x/(1-x)), 0, 0.1)
curve(log(x), 0, 0.1)


par(mfrow=c(1, 2))
curve(log(x/(1-x)), 0, 1)
curve(log(x), 0, 1)

(RR = (77 / 350) / (61 / 350))
(OR = (77 / 273) / (61 / 289))
77/350
61/350
138/700

# assuming google is gold standard, i.e., measured without error
# simulate 100 measures from a normal(100, 10) dist
google <- rnorm(100, mean = 100, sd = 10)

# bing.measures is a function of the google measures, with some added noise
bing.measures <- google + rnorm(100, mean = 10, sd = 2)

# very high correlation between the two apps
plot(google, bing.measures)
cor(google, bing.measures)

# regression shows an adjusted R^2 greater than 0.95
summary(lm(bing.measures~google))

# however if we plot the measures, bing is higher than google:
boxplot(measures ~ bing)

# regression for the delta mu confirms this
# a significant difference between google measures and bing measures
measures <- c(google, bing.measures)
bing <- c(rep(0,100), rep(1, 100))
summary(lm(measures ~ bing))

# moral of the story. just because two measures are almost perfectly correlated
# does not mean the measurement values themselves are similar!

